"""
FastAPI server for workflow execution.
Generated by configurable-agents deploy command.

Workflow: ${workflow_name}
Sync timeout: ${sync_timeout}s
"""
import asyncio
import os
import time
import uuid
from datetime import datetime
from typing import Any, Dict

from fastapi import BackgroundTasks, FastAPI, HTTPException
from pydantic import BaseModel, Field, create_model, ValidationError

from configurable_agents.config import WorkflowConfig, parse_config_file
from configurable_agents.runtime import run_workflow_from_config

# Agent registry integration (optional)
${registry_import}

# MLFlow integration (optional)
MLFLOW_ENABLED = os.getenv("MLFLOW_TRACKING_URI") is not None
if MLFLOW_ENABLED:
    import mlflow

# ==================================
# Configuration
# ==================================
WORKFLOW_CONFIG_PATH = "workflow.yaml"
SYNC_TIMEOUT = ${sync_timeout}  # seconds

# Load workflow config at startup (fail-fast)
try:
    config_dict = parse_config_file(WORKFLOW_CONFIG_PATH)
    workflow_config = WorkflowConfig(**config_dict)
except Exception as e:
    print(f"FATAL: Failed to load workflow config: {e}")
    raise

${registry_client_init}


# ==================================
# Input Validation Model
# ==================================
def _build_input_model():
    """Build Pydantic model from workflow state schema for input validation"""
    fields = {}

    # Map workflow field types to Python types
    type_map = {
        "str": str,
        "int": int,
        "float": float,
        "bool": bool,
        "list": list,
        "dict": dict,
    }

    for field_name, field_def in workflow_config.state.fields.items():
        # Parse type (handles list[T], dict, etc.)
        field_type_str = field_def.type
        if field_type_str in type_map:
            python_type = type_map[field_type_str]
        elif field_type_str.startswith("list["):
            python_type = list
        elif field_type_str.startswith("dict"):
            python_type = dict
        else:
            python_type = Any

        # Build field with default or required
        if field_def.required:
            fields[field_name] = (python_type, Field(description=field_def.description or ""))
        else:
            default_value = field_def.default if field_def.default is not None else None
            fields[field_name] = (python_type, Field(default=default_value, description=field_def.description or ""))

    return create_model("WorkflowInput", **fields)


# Create input validation model
WorkflowInput = _build_input_model()


# ==================================
# Job Store (In-Memory, v0.1)
# ==================================
jobs: Dict[str, Dict[str, Any]] = {}


# ==================================
# Models
# ==================================
class RunResponse(BaseModel):
    """Response for /run endpoint"""
    status: str
    execution_time_ms: int | None = None
    outputs: Dict[str, Any] | None = None
    job_id: str | None = None
    message: str | None = None


class JobStatusResponse(BaseModel):
    """Response for /status endpoint"""
    job_id: str
    status: str
    created_at: str
    completed_at: str | None = None
    execution_time_ms: int | None = None
    outputs: Dict[str, Any] | None = None
    error: str | None = None


class HealthResponse(BaseModel):
    """Response for /health endpoint"""
    status: str
    timestamp: str


class SchemaResponse(BaseModel):
    """Response for /schema endpoint"""
    workflow: str
    inputs: Dict[str, Any]
    outputs: list[str]


# ==================================
# FastAPI App
# ==================================
app = FastAPI(
    title="${workflow_name} API",
    description="Workflow API generated by configurable-agents",
    version="1.0.0"
)


# ==================================
# Background Task
# ==================================
async def run_workflow_async(job_id: str, inputs: Dict[str, Any]):
    """Run workflow in background and update job store"""
    start_time = time.time()
    jobs[job_id]["status"] = "running"

    # Start MLFlow run if enabled
    mlflow_run_id = None
    if MLFLOW_ENABLED:
        try:
            mlflow.set_experiment("${workflow_name}")
            run = mlflow.start_run(run_name=f"async_{job_id[:8]}")
            mlflow_run_id = run.info.run_id
            mlflow.log_params(inputs)
        except Exception as e:
            print(f"Warning: MLFlow tracking failed: {e}")

    try:
        result = await asyncio.to_thread(run_workflow_from_config, workflow_config, inputs)
        execution_time_ms = int((time.time() - start_time) * 1000)

        # Log metrics to MLFlow
        if MLFLOW_ENABLED and mlflow_run_id:
            try:
                mlflow.log_metrics({
                    "execution_time_ms": execution_time_ms,
                    "success": 1
                })
                mlflow.end_run()
            except Exception as e:
                print(f"Warning: MLFlow logging failed: {e}")

        jobs[job_id].update({
            "status": "completed",
            "completed_at": datetime.utcnow().isoformat(),
            "execution_time_ms": execution_time_ms,
            "outputs": result,
            "error": None
        })
    except Exception as e:
        # Log failure to MLFlow
        if MLFLOW_ENABLED and mlflow_run_id:
            try:
                mlflow.log_metrics({"success": 0})
                mlflow.log_param("error", str(e))
                mlflow.end_run(status="FAILED")
            except Exception as mlflow_err:
                print(f"Warning: MLFlow logging failed: {mlflow_err}")

        jobs[job_id].update({
            "status": "failed",
            "completed_at": datetime.utcnow().isoformat(),
            "error": str(e)
        })


# ==================================
# Lifecycle Events
# ==================================
${registry_startup_handler}
${registry_shutdown_handler}

# ==================================
# Endpoints
# ==================================
@app.get("/")
async def root():
    """API information"""
    return {
        "workflow": "${workflow_name}",
        "status": "running",
        "endpoints": {
            "run": "/run (POST)",
            "status": "/status/{job_id} (GET)",
            "health": "/health (GET)",
            "schema": "/schema (GET)",
            "docs": "/docs (GET)"
        }
    }


@app.post("/run", response_model=RunResponse)
async def run_workflow_endpoint(inputs: WorkflowInput, background_tasks: BackgroundTasks):
    """
    Execute workflow with sync/async fallback.

    - If completes within ${sync_timeout}s -> returns outputs immediately (sync)
    - If exceeds ${sync_timeout}s -> returns job_id for polling (async)

    Inputs are validated against workflow schema.
    """
    start_time = time.time()

    # Convert validated Pydantic model to dict
    inputs_dict = inputs.model_dump()

    # Start MLFlow run if enabled (sync mode)
    mlflow_run_id = None
    if MLFLOW_ENABLED:
        try:
            mlflow.set_experiment("${workflow_name}")
            run = mlflow.start_run(run_name=f"sync_{uuid.uuid4().hex[:8]}")
            mlflow_run_id = run.info.run_id
            mlflow.log_params(inputs_dict)
        except Exception as e:
            print(f"Warning: MLFlow tracking failed: {e}")

    try:
        # Attempt sync execution with timeout
        result = await asyncio.wait_for(
            asyncio.to_thread(run_workflow_from_config, workflow_config, inputs_dict),
            timeout=SYNC_TIMEOUT
        )

        execution_time_ms = int((time.time() - start_time) * 1000)

        # Log success to MLFlow
        if MLFLOW_ENABLED and mlflow_run_id:
            try:
                mlflow.log_metrics({
                    "execution_time_ms": execution_time_ms,
                    "success": 1
                })
                mlflow.end_run()
            except Exception as e:
                print(f"Warning: MLFlow logging failed: {e}")

        return RunResponse(
            status="success",
            execution_time_ms=execution_time_ms,
            outputs=result
        )

    except asyncio.TimeoutError:
        # End MLFlow run for sync attempt
        if MLFLOW_ENABLED and mlflow_run_id:
            try:
                mlflow.log_param("timeout", True)
                mlflow.end_run(status="FINISHED")
            except Exception as e:
                print(f"Warning: MLFlow logging failed: {e}")

        # Workflow too slow -> async mode
        job_id = str(uuid.uuid4())
        jobs[job_id] = {
            "status": "pending",
            "created_at": datetime.utcnow().isoformat()
        }
        background_tasks.add_task(run_workflow_async, job_id, inputs_dict)

        return RunResponse(
            status="async",
            job_id=job_id,
            message=f"Workflow exceeds {SYNC_TIMEOUT}s timeout. Poll /status/{job_id} for results."
        )

    except Exception as e:
        # Log failure to MLFlow
        if MLFLOW_ENABLED and mlflow_run_id:
            try:
                mlflow.log_metrics({"success": 0})
                mlflow.log_param("error", str(e))
                mlflow.end_run(status="FAILED")
            except Exception as mlflow_err:
                print(f"Warning: MLFlow logging failed: {mlflow_err}")

        raise HTTPException(status_code=500, detail=f"Workflow execution failed: {str(e)}")


@app.get("/status/{job_id}", response_model=JobStatusResponse)
async def get_job_status(job_id: str):
    """Get async job status"""
    if job_id not in jobs:
        raise HTTPException(status_code=404, detail="Job not found")

    job = jobs[job_id]
    return JobStatusResponse(job_id=job_id, **job)


@app.get("/health", response_model=HealthResponse)
async def health():
    """Health check for orchestration (k8s, ECS)"""
    return HealthResponse(
        status="healthy",
        timestamp=datetime.utcnow().isoformat()
    )


@app.get("/schema", response_model=SchemaResponse)
async def schema():
    """Return workflow input/output schema"""
    # Extract state fields (inputs)
    inputs = {}
    for field_name, field_def in workflow_config.state.fields.items():
        inputs[field_name] = {
            "type": field_def.type,
            "required": field_def.required,
            "default": field_def.default,
            "description": field_def.description or ""
        }

    # Extract outputs (state fields written by nodes)
    outputs = list(workflow_config.state.fields.keys())

    return SchemaResponse(
        workflow=workflow_config.flow.name,
        inputs=inputs,
        outputs=outputs
    )
${health_check_endpoints}

# ==================================
# Server Startup
# ==================================
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=${api_port})
