---
phase: 03-interfaces-and-triggers
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/configurable_agents/storage/models.py
  - src/configurable_agents/storage/base.py
  - src/configurable_agents/storage/sqlite.py
  - src/configurable_agents/storage/factory.py
  - src/configurable_agents/storage/__init__.py
  - src/configurable_agents/ui/__init__.py
  - src/configurable_agents/ui/gradio_chat.py
  - src/configurable_agents/llm/__init__.py
  - tests/ui/test_gradio_chat.py
  - tests/storage/test_chat_session_repository.py
autonomous: true
user_setup: []

must_haves:
  truths:
    - "User can describe a desired workflow in natural language through the Gradio chat interface and receive a valid, runnable YAML config"
    - "User can close the browser, reopen the chat UI, and continue a previous config generation conversation where they left off"
    - "Chat UI validates generated configs against WorkflowConfig schema before presenting to user"
    - "Session history persists to SQLite database for cross-device recovery"
  artifacts:
    - path: "src/configurable_agents/storage/models.py"
      provides: "ChatSession and ChatMessage ORM models"
      contains: "class ChatSession", "class ChatMessage"
    - path: "src/configurable_agents/storage/base.py"
      provides: "ChatSessionRepository abstract interface"
      exports: ["ChatSessionRepository"]
    - path: "src/configurable_agents/ui/gradio_chat.py"
      provides: "Gradio ChatInterface for config generation"
      min_lines: 100
    - path: "src/configurable_agents/llm/__init__.py"
      provides: "LLM client for config generation"
      exports: ["create_llm", "stream_chat"]
  key_links:
    - from: "src/configurable_agents/ui/gradio_chat.py"
      to: "src/configurable_agents/storage/base.py"
      via: "ChatSessionRepository for session persistence"
      pattern: "ChatSessionRepository"
    - from: "src/configurable_agents/ui/gradio_chat.py"
      to: "src/configurable_agents/config/schema.py"
      via: "WorkflowConfig validation"
      pattern: "WorkflowConfig"
    - from: "src/configurable_agents/ui/gradio_chat.py"
      to: "src/configurable_agents/llm"
      via: "LLM streaming for config generation"
      pattern: "stream_chat|stream\\(\\)"
---

<objective>
Build a Gradio-based conversational UI for generating valid workflow configs through natural language interaction, with SQLite-backed session persistence for cross-device conversation recovery.

Purpose: Enable users to create complex YAML configs without manual editing by describing their desired workflow in plain English. The UI validates generated configs against the WorkflowConfig schema and maintains conversation history across browser sessions.

Output: Gradio chat UI server on port 7860, chat session storage models and repositories, config generation prompt template
</objective>

<execution_context>
@C:\Users\ghost\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\ghost\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-interfaces-and-triggers/03-RESEARCH.md
@.planning/phases/01-core-engine/01-02-SUMMARY.md
@.planning/phases/02-agent-infrastructure/02-01A-SUMMARY.md

@src/configurable_agents/config/schema.py
@src/configurable_agents/runtime/executor.py
@src/configurable_agents/storage/models.py
@src/configurable_agents/storage/base.py
@src/configurable_agents/llm/__init__.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create chat session storage layer</name>
  <files>src/configurable_agents/storage/models.py, src/configurable_agents/storage/base.py, src/configurable_agents/storage/sqlite.py, src/configurable_agents/storage/factory.py, src/configurable_agents/storage/__init__.py</files>
  <action>
    Add chat session storage extending the existing Repository Pattern from Phase 1:

    1. **Add ORM models to models.py:**
       - ChatSession model with fields: session_id (String(36) PK), user_identifier (String(256), indexed), created_at, updated_at, generated_config (Text, nullable), status (String(32), default="in_progress")
       - ChatMessage model with fields: id (Integer, autoincrement PK), session_id (FK to chat_sessions), role (String(32): "user" or "assistant"), content (Text), created_at, metadata (Text, nullable JSON)
       - Use SQLAlchemy 2.0 Mapped/mapped_column pattern matching existing models

    2. **Add ChatSessionRepository interface to base.py:**
       - create_session(user_identifier: str) -> str: Create new session, return session_id
       - get_session(session_id: str) -> Optional[Dict]: Get session by ID
       - add_message(session_id: str, role: str, content: str, metadata: Optional[Dict]) -> None: Add message to session
       - get_messages(session_id: str) -> List[Dict]: Get all messages for session ordered by created_at
       - update_config(session_id: str, config_yaml: str) -> None: Save generated config
       - list_recent_sessions(user_identifier: str, limit: int = 10) -> List[Dict]: List recent sessions

    3. **Add SQLiteChatSessionRepository to sqlite.py:**
       - Implement all ChatSessionRepository methods using AsyncSession
       - Use select() queries with proper ordering (created_at ASC for messages)
       - Handle WAL mode for concurrent access (PRAGMA journal_mode=WAL)
       - Follow existing patterns from SqliteWorkflowRunRepository

    4. **Update factory.py and __init__.py:**
       - Modify create_storage_backend() to return tuple of 5 repos (add chat_session_repo to existing 4)
       - Export ChatSessionRepository and SQLiteChatSessionRepository from storage __init__

    Reference research: .planning/phases/03-interfaces-and-triggers/03-RESEARCH.md lines 888-1044
    Use existing patterns from 01-01A-SUMMARY for SQLAlchemy 2.0 async patterns
  </action>
  <verify>
    from configurable_agents.storage import create_storage_backend, ChatSessionRepository
    workflow_repo, state_repo, agent_repo, cost_repo, chat_repo = create_storage_backend(None)
    session_id = await chat_repo.create_session("test-user")
    await chat_repo.add_message(session_id, "user", "Hello")
    messages = await chat_repo.get_messages(session_id)
    assert len(messages) == 1
    assert messages[0]["role"] == "user"
  </verify>
  <done>
    ChatSession and ChatMessage ORM models exist with correct schema
    ChatSessionRepository abstract interface defined with all required methods
    SQLiteChatSessionRepository implements all methods using async SQLAlchemy patterns
    create_storage_backend() returns 5-tuple including chat_session_repo
    All types exported from storage __init__
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Gradio chat UI with config generation</name>
  <files>src/configurable_agents/ui/__init__.py, src/configurable_agents/ui/gradio_chat.py, src/configurable_agents/llm/__init__.py</files>
  <action>
    Create Gradio ChatInterface for conversational config generation:

    1. **Create ui package structure:**
       - src/configurable_agents/ui/__init__.py: Exports for GradioChatUI
       - src/configurable_agents/ui/gradio_chat.py: Main chat interface implementation

    2. **Add LLM streaming support to llm/__init__.py:**
       - Export stream_chat() function that yields response chunks
       - Reuse existing create_llm() from Phase 1
       - Support streaming for all providers (OpenAI, Anthropic, Google, Ollama)

    3. **Create GradioChatUI class in gradio_chat.py:**
       - __init__(llm_client, session_repo, config_schema): Initialize with LLM client, session repository, WorkflowConfig schema
       - CONFIG_GENERATION_PROMPT: System prompt for YAML config generation (includes schema v1.0 format reference from config/schema.py)

       - _build_conversation_context(history: List[Tuple[str, str]]) -> str: Format last 5 messages into context string

       - _extract_yaml_block(text: str) -> Optional[str]: Use regex to extract YAML from markdown code blocks

       - _validate_generated_config(yaml_content: str) -> Tuple[bool, str]: Parse YAML, validate against WorkflowConfig schema, return (is_valid, error_or_config)

       - generate_config(message: str, history: List[Tuple[str, str]], request: gr.Request) -> Generator[str, None, None]:
         * Derive session_id from request.client.host + request.client.port (or from gr.Request state)
         * Load session history from session_repo
         * Build conversation context
         * Stream LLM response chunk by chunk
         * On completion: extract YAML, validate against schema, save to session_repo
         * Yield formatted response with YAML block and download instructions

       - download_config(history: List[Tuple[str, str]]) -> Optional[str]: Extract last generated config to temp file for download

       - validate_config(history: List[Tuple[str, str]]) -> str: Validate last generated config, return status message

       - create_interface() -> gr.Blocks: Build Gradio ChatInterface with:
         * ChatInterface component with generate_config as fn
         * Examples: "Research a topic and write a 500-word article with sources", "Analyze sentiment of customer reviews and categorize them", "Summarize a long document into bullet points"
         * Additional action buttons: Download YAML, Validate Config
         * Custom CSS for clean UI (no emoji)

       - launch(server_name: str = "0.0.0.0", server_port: int = 7860, share: bool = False): Launch Gradio server

    4. **Session persistence pattern:**
       - Use browser LocalStorage for immediate persistence via Gradio's custom JavaScript
       - Back up to SQLite on each message via session_repo
       - Load previous session on reconnect via session_id

    Reference research: .planning/phases/03-interfaces-and-triggers/03-RESEARCH.md lines 100-231, 1168-1347
    Use existing WorkflowConfig from config/schema.py for validation
    Use existing LLM providers from llm/__init__.py (Phase 1, 01-02-SUMMARY)
  </action>
  <verify>
    from configurable_agents.ui import GradioChatUI
    from configurable_agents.storage import create_storage_backend
    from configurable_agents.llm import create_llm

    # Create dependencies
    _, _, _, _, chat_repo = create_storage_backend(None)
    llm_client = create_llm(None, None)
    ui = GradioChatUI(llm_client, chat_repo, None)

    # Verify interface creation
    interface = ui.create_interface()
    assert interface is not None
    # Verify has expected methods
    assert hasattr(ui, 'generate_config')
    assert hasattr(ui, 'download_config')
    assert hasattr(ui, 'launch')
  </verify>
  <done>
    GradioChatUI class created with all required methods
    generate_config() streams LLM responses and validates YAML output
    Session persistence working via ChatSessionRepository
    Config generation prompt includes schema v1.0 format reference
    ChatInterface launches on port 7860 with action buttons
  </done>
</task>

<task type="auto">
  <name>Task 3: Add tests for chat session storage and Gradio UI</name>
  <files>tests/storage/test_chat_session_repository.py, tests/ui/test_gradio_chat.py</files>
  <action>
    Create comprehensive tests for chat session storage and Gradio UI:

    1. **tests/storage/test_chat_session_repository.py:**
       - test_create_session: Verifies session creation returns valid UUID
       - test_add_and_get_messages: Verifies messages are stored and retrieved in order
       - test_update_config: Verifies generated config is persisted
       - test_list_recent_sessions: Verifies recent sessions are returned with limit
       - test_get_nonexistent_session: Verifies None returned for invalid session_id
       - test_concurrent_message_add: SQLite WAL mode concurrent writes (2 coroutines)

    2. **tests/ui/test_gradio_chat.py:**
       - test_conversation_context_building: Verifies history is formatted correctly
       - test_yaml_extraction: Verifies regex extracts YAML from markdown blocks
       - test_config_validation: Verifies WorkflowConfig schema validation
       - test_config_validation_invalid_yaml: Verifies errors are caught
       - test_gradio_ui_creation: Verifies GradioChatUI creates interface successfully
       - test_session_persistence: Mock session_repo, verify session_id derived from request

    Use pytest with pytest.mark.asyncio for async tests
    Mock gr.Request for request-dependent tests
    Use existing test patterns from 01-01A, 02-01A for SQLAlchemy testing
  </action>
  <verify>
    pytest tests/storage/test_chat_session_repository.py -v
    pytest tests/ui/test_gradio_chat.py -v
    # All tests pass, coverage > 80% for new code
  </verify>
  <done>
    All tests passing
    Test coverage > 80% for chat session storage and Gradio UI code
    Async tests properly decorated with pytest.mark.asyncio
    Mock patterns consistent with existing tests
  </done>
</task>

</tasks>

<verification>
After completing all tasks, verify:

1. **Storage layer tests pass:**
   ```bash
   pytest tests/storage/test_chat_session_repository.py -v
   ```

2. **Gradio UI tests pass:**
   ```bash
   pytest tests/ui/test_gradio_chat.py -v
   ```

3. **Import verification:**
   ```bash
   python -c "from configurable_agents.ui import GradioChatUI; from configurable_agents.storage import ChatSessionRepository; print('OK')"
   ```

4. **Manual verification (checkpoint):**
   - Launch chat UI: `python -m configurable_agents.ui.gradio_chat`
   - Visit http://localhost:7860
   - Send a message describing a workflow
   - Verify YAML config is generated and validated
   - Refresh page and verify conversation history persists
</verification>

<success_criteria>
1. Gradio chat UI launches on port 7860
2. User can describe workflow in natural language and receive valid YAML config
3. Generated configs pass WorkflowConfig schema validation
4. Conversation history persists across browser refreshes
5. Session history stored in SQLite chat_sessions and chat_messages tables
6. Download button exports generated YAML file
7. Validate button confirms config schema validity
8. All automated tests pass with >80% coverage
</success_criteria>

<output>
After completion, create `.planning/phases/03-interfaces-and-triggers/03-01-SUMMARY.md`
</output>
