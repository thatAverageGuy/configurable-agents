---
phase: 04-advanced-capabilities
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/configurable_agents/storage/base.py
  - src/configurable_agents/storage/sqlite.py
  - src/configurable_agents/storage/models.py
  - src/configurable_agents/memory/__init__.py
  - src/configurable_agents/memory/store.py
  - src/configurable_agents/tools/registry.py
  - src/configurable_agents/tools/web_tools.py
  - src/configurable_agents/tools/file_tools.py
  - src/configurable_agents/tools/data_tools.py
  - src/configurable_agents/tools/system_tools.py
  - src/configurable_agents/config/schema.py
  - src/configurable_agents/core/node_executor.py
  - tests/memory/test_store.py
  - tests/tools/test_web_tools.py
  - tests/tools/test_file_tools.py
  - tests/tools/test_data_tools.py
  - examples/memory_example.yaml
  - examples/tools_example.yaml
autonomous: true
user_setup:
  - service: serper
    why: "Web search tool requires API key"
    env_vars:
      - name: SERPER_API_KEY
        source: "https://serper.dev/api-key"
      - name: WEB_SEARCH_PROVIDER
        source: "Set to 'serper' or 'tavily' (optional, defaults to serper)"
    dashboard_config:
      - task: "Create Serper account and get API key"
        location: "https://serper.dev"

must_haves:
  truths:
    - "User can write memory via agent.memory.write() and read via agent.memory['key']"
    - "Memory persists across workflow runs (same agent/workflow scope)"
    - "Memory keys are namespaced by agent/workflow/node to prevent conflicts"
    - "User can add pre-built tools to nodes via YAML tools list"
    - "Tool results are structured Python types (dict, list, str, int)"
    - "Tool errors fail the node unless on_error: 'continue' is specified"
  artifacts:
    - path: "src/configurable_agents/memory/store.py"
      provides: "Persistent memory storage with namespaced keys"
      exports: ["MemoryStore", "AgentMemory", "memory_context"]
      min_lines: 150
    - path: "src/configurable_agents/tools/web_tools.py"
      provides: "Web tools (search, scrape, http_client)"
      exports: ["web_search", "web_scrape", "http_client"]
      min_lines: 100
    - path: "src/configurable_agents/tools/file_tools.py"
      provides: "File I/O tools (read, write, glob, move)"
      exports: ["file_read", "file_write", "file_glob", "file_move"]
      min_lines: 100
    - path: "src/configurable_agents/tools/data_tools.py"
      provides: "Data processing tools (sql, csv, json, yaml)"
      exports: ["sql_query", "dataframe_to_csv", "json_parse", "yaml_parse"]
      min_lines: 100
    - path: "src/configurable_agents/storage/models.py"
      provides: "MemoryRecord ORM model"
      contains: "class MemoryRecord"
  key_links:
    - from: "src/configurable_agents/core/node_executor.py"
      to: "src/configurable_agents/memory"
      via: "Create AgentMemory instance from node config, inject into node execution context"
      pattern: "from configurable_agents.memory import.*AgentMemory"
    - from: "src/configurable_agents/core/node_executor.py"
      to: "src/configurable_agents/tools"
      via: "Bind tools from node.tools config to LLM via bind_tools()"
      pattern: "bind_tools.*get_tool"
    - from: "src/configurable_agents/memory/store.py"
      to: "src/configurable_agents/storage"
      via: "MemoryRepository interface for persistent storage"
      pattern: "from configurable_agents.storage import.*MemoryRepository"
---

<objective>
Implement long-term persistent memory for agents and expand the tool ecosystem with essential pre-built tools for web access, file I/O, data processing, and system operations.

Purpose: Enable agents to maintain context across workflow executions (RT-07) and provide a curated set of high-value tools (RT-08) that users can configure via YAML without writing code.

Output: Namespaced memory storage with configurable scope and 12+ pre-built tools across four categories (web, file, data, system).
</objective>

<execution_context>
@C:\Users\ghost\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\ghost\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-advanced-capabilities/04-CONTEXT.md
@src/configurable_agents/storage/base.py
@src/configurable_agents/storage/models.py
@src/configurable_agents/tools/registry.py
@src/configurable_agents/config/schema.py
@src/configurable_agents/core/node_executor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create persistent memory storage with namespaced keys</name>
  <files>
    src/configurable_agents/memory/__init__.py
    src/configurable_agents/memory/store.py
    src/configurable_agents/storage/base.py
    src/configurable_agents/storage/sqlite.py
    src/configurable_agents/storage/models.py
    tests/memory/test_store.py
  </files>
  <action>
    Create persistent memory system:

    1. **src/configurable_agents/storage/models.py** - Add memory ORM:
       - Add `MemoryRecord` model (use SQLAlchemy 2.0 pattern):
         - `id: int` (primary key, auto-increment)
         - `agent_id: str` (indexed)
         - `workflow_id: str | None` (indexed)
         - `node_id: str | None` (indexed)
         - `key: str` (indexed)
         - `namespace_key: str` (unique, indexed) - combined "agent_id:workflow_id:node_id:key"
         - `value: str` (JSON serialized)
         - `created_at: datetime` (default: func.now())
         - `updated_at: datetime` (onupdate: func.now())
       - Use Mapped and mapped_column from SQLAlchemy 2.0

    2. **src/configurable_agents/storage/base.py** - Add memory repository interface:
       - Add `MemoryRepository` abstract base class:
         - `get(namespace_key: str) -> str | None`
         - `set(namespace_key: str, value: str, agent_id: str, workflow_id: str | None, node_id: str | None) -> None`
         - `delete(namespace_key: str) -> bool`
         - `list(agent_id: str, prefix: str = "") -> list[tuple[str, str]]` (returns (key, value) pairs)
         - `clear(agent_id: str) -> int` (returns count cleared)
         - `clear_by_workflow(agent_id: str, workflow_id: str) -> int`

    3. **src/configurable_agents/storage/sqlite.py** - Implement memory repository:
       - Add `SQLiteMemoryRepository(MemoryRepository)`:
         - Use existing Session pattern from other repositories
         - Implement all methods with proper error handling
         - Use `INSERT OR REPLACE` for set() (upsert semantics)
         - Namespace format: `{agent_id}:{workflow_id or "*"}:{node_id or "*"}:{key}`
         - Filter by agent/workflow/node in list() and clear()

    4. **Update storage factory** - Add memory repo creation:
       - Modify `create_storage_backend()` to also create memory repository
       - Return memory repository alongside existing repos

    5. **src/configurable_agents/memory/store.py** - Memory API:
       - `MemoryStore` class for direct storage access
       - `AgentMemory` class with dict-like read + explicit write:
         - `__init__(agent_id: str, workflow_id: str | None = None, node_id: str | None = None, scope: Literal["agent", "workflow", "node"] = "agent", repo: MemoryRepository)`
         - `__getitem__(key: str) -> Any` - dict-like read with auto-deserialization
         - `write(key: str, value: Any, ttl: int | None = None) -> None` - serialize to JSON, store via repo
         - `read(key: str, default: Any = None) -> Any` - explicit read with default
         - `delete(key: str) -> bool` - remove key
         - `list(prefix: str = "") -> list[tuple[str, Any]]` - list keys with prefix
         - `clear() -> None` - clear all memory at current scope
         - `_build_namespace(key: str) -> str` - private method for namespace construction
       - Use `json.loads()`/`json.dumps()` for value serialization
       - Scope logic:
         - "agent": namespace includes agent_id only (shared across all workflows)
         - "workflow": namespace includes agent_id:workflow_id (shared across nodes)
         - "node": namespace includes agent_id:workflow_id:node_id (isolated)

    6. **src/configurable_agents/memory/__init__.py** - Public API:
       - Export `MemoryStore`, `AgentMemory`, `memory_context` (context manager for cleanup)

    7. **tests/memory/test_store.py** - Test suite (min 200 lines):
       - Test AgentMemory dict-like reads
       - Test write with JSON serialization
       - Test namespace isolation (agent vs workflow vs node scope)
       - Test list with prefix filtering
       - Test clear operations
       - Test SQLite repository implementation
       - Use existing storage fixture patterns from conftest

    Follow existing storage patterns:
    - Use context managers for session handling
    - Graceful degradation if storage unavailable
    - Log operations at DEBUG level
  </action>
  <verify>
    Run tests: pytest tests/memory/test_store.py -v
    Verify memory table created: check sqlite schema for memory_records table
  </verify>
  <done>
    AgentMemory provides dict-like reads and explicit writes with namespaced keys. Memory persists to SQLite via repository pattern. Scope isolation (agent/workflow/node) prevents key conflicts.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement web and file tool ecosystem</name>
  <files>
    src/configurable_agents/tools/web_tools.py
    src/configurable_agents/tools/file_tools.py
    src/configurable_agents/tools/registry.py
    pyproject.toml
    tests/tools/test_web_tools.py
    tests/tools/test_file_tools.py
  </files>
  <action>
    Create web and file tools:

    1. **pyproject.toml** - Add tool dependencies:
       - `beautifulsoup4>=4.12.0` (for web_scrape)
       - `requests>=2.31.0` (for http_client, file_tools)
       - `aiohttp>=3.9.0` is optional for async

    2. **src/configurable_agents/tools/web_tools.py** - Web tools:
       - `web_search` tool (rename and enhance existing serper.py):
         - Accept query: str, num_results: int = 10
         - Read WEB_SEARCH_PROVIDER env var (default: "serper")
         - Support serper.dev and tavily.com APIs
         - Return dict: {results: [{title, url, snippet}]}
       - `web_scrape` tool:
         - Accept url: str, selector: str | None = None
         - Use BeautifulSoup to extract text content
         - If selector provided, use CSS selector for targeted extraction
         - Return dict: {url, title, content, extracted_text}
         - Handle HTTP errors, parse errors gracefully
       - `http_client` tool:
         - Accept method: str, url: str, headers: dict | None = None, body: Any | None = None
         - Use requests library for HTTP calls
         - Return dict: {status_code, headers, body, error}
         - Support GET, POST, PUT, DELETE methods
       - Register all tools in ToolRegistry

    3. **src/configurable_agents/tools/file_tools.py** - File I/O tools:
       - `file_read` tool:
         - Accept path: str, encoding: str = "utf-8"
         - Return dict: {path, content, size, error}
         - Restrict to safe directories: use ALLOWED_PATHS env var or cwd
         - Reject paths with .. for security
       - `file_write` tool:
         - Accept path: str, content: str, encoding: str = "utf-8"
         - Create parent dirs if not exist
         - Return dict: {path, bytes_written, error}
         - Same security restrictions as file_read
       - `file_glob` tool:
         - Accept pattern: str, base_path: str = "."
         - Use pathlib.Path.glob() for matching
         - Return dict: {pattern, matches: [paths]}
         - Restrict to safe directories
       - `file_move` tool:
         - Accept source: str, dest: str
         - Use pathlib.Path.rename() for atomic move
         - Return dict: {source, dest, success, error}
       - Register all tools in ToolRegistry

    4. **src/configurable_agents/tools/registry.py** - Enhance registry:
       - Add `get_tools_config()` method to return tool metadata
       - Add `on_error` support: track error handling mode per tool
       - Add `create_tool_from_config()` for YAML-based tool config
       - Support both simple string names and dict configs in registration

    5. **tests/tools/test_web_tools.py** - Test suite (min 150 lines):
       - Mock HTTP requests for web_scrape and http_client
       - Test web_search with mock API responses
       - Test error handling (HTTP 404, timeout, invalid URL)
       - Mark API tests as @pytest.mark.integration

    6. **tests/tools/test_file_tools.py** - Test suite (min 150 lines):
       - Use tmp_path fixture for file operations
       - Test read/write with various encodings
       - Test glob pattern matching
       - Test file move and rename
       - Test security restrictions (.. paths, outside allowed dirs)

    Follow tool patterns:
    - Return structured dicts (not strings)
    - Include error field in all tool results
    - Use LangChain Tool decorator pattern
    - Register tools via @tool decorator or manual Tool.from_function
  </action>
  <verify>
    Run tests: pytest tests/tools/test_web_tools.py tests/tools/test_file_tools.py -v
    Verify tools registered: python -c "from configurable_agents.tools import list_tools; print(list_tools())"
  </verify>
  <done>
    Web tools (search, scrape, http_client) and file tools (read, write, glob, move) implemented as LangChain tools. All tools return structured results with error handling. Tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 3: Implement data and system tools, integrate memory and tools into node executor</name>
  <files>
    src/configurable_agents/tools/data_tools.py
    src/configurable_agents/tools/system_tools.py
    src/configurable_agents/config/schema.py
    src/configurable_agents/core/node_executor.py
    tests/tools/test_data_tools.py
    examples/memory_example.yaml
    examples/tools_example.yaml
  </files>
  <action>
    Create data/system tools and wire up integration:

    1. **src/configurable_agents/tools/data_tools.py** - Data processing tools:
       - `sql_query` tool:
         - Accept query: str, connection_string: str (sqlite by default)
         - Use sqlite3 for in-memory SQLite queries
         - Return dict: {rows: [[values]], columns: [names], row_count}
         - Support SELECT only (reject DROP, DELETE, UPDATE, INSERT for safety)
       - `dataframe_to_csv` tool:
         - Accept data: list[dict] | dict[str, list], path: str
         - Create pandas DataFrame from input
         - Write to CSV file
         - Return dict: {path, rows_written}
         - Add pandas to pyproject.toml
       - `json_parse` tool:
         - Accept json_string: str
         - Parse JSON and return dict
         - Return dict: {parsed: Any, error: str | None}
       - `yaml_parse` tool:
         - Accept yaml_string: str
         - Parse YAML and return dict
         - Return dict: {parsed: Any, error: str | None}
       - Register all tools

    2. **src/configurable_agents/tools/system_tools.py** - System tools:
       - `shell` tool:
         - Accept command: str, timeout: int = 30
         - Use subprocess.run() with timeout
         - Return dict: {stdout, stderr, returncode, error}
         - Security: Restrict allowed commands via ALLOWED_COMMANDS env var
       - `process` tool:
         - Accept command: str, args: list[str]
         - Spawn background process with subprocess.Popen
         - Return dict: {pid, status}
         - Store process handles for later status checks
       - `env_vars` tool:
         - Accept pattern: str | None = None
         - Return dict: {vars: {key: value}, error}
         - Filter by pattern if provided (e.g., "PATH*")
         - Exclude sensitive vars (API_KEY, SECRET, PASSWORD, TOKEN)
       - Register all tools

    3. **pyproject.toml** - Add dependencies:
       - `pandas>=2.0.0` (for dataframe_to_csv)
       - `pyyaml>=6.0` (for yaml_parse)

    4. **src/configurable_agents/config/schema.py** - Extend schema for memory and tools:
       - Add `MemoryConfig` model:
         - `enabled: bool = False`
         - `default_scope: Literal["agent", "workflow", "node"] = "agent"`
       - Add `ToolConfig` model:
         - `name: str`
         - `on_error: Literal["fail", "continue"] = "fail"`
         - `config: dict | None = None`
       - Add `memory: MemoryConfig | None = None` to `WorkflowConfig`
       - Add `memory: MemoryConfig | None = None` to `NodeConfig` (override workflow default)
       - Add `tools: list[str | ToolConfig] = []` to `NodeConfig`

    5. **src/configurable_agents/core/node_executor.py** - Wire up memory and tools:
       - Import memory: `from configurable_agents.memory import AgentMemory`
       - In execute_node, create AgentMemory if memory.enabled:
         - Get agent_id from workflow config or generate from workflow name
         - Get workflow_id from current execution context
         - Get node_id from node config
         - Get scope from node.memory.default_scope or workflow.memory.default_scope
         - Pass memory repo from storage backend
       - Inject memory into node execution globals (for code execution nodes)
       - Bind tools to LLM:
         - Resolve tool names from node.tools config
         - Get tools from ToolRegistry
         - Call `llm.bind_tools(tools)` before invoking
       - Handle tool errors based on on_error config:
         - "fail": raise exception, mark node as failed
         - "continue": catch error, return error result, continue workflow

    6. **tests/tools/test_data_tools.py** - Test suite (min 100 lines):
       - Test sql_query with in-memory SQLite
       - Test dataframe_to_csv with various input formats
       - Test json_parse with valid and invalid JSON
       - Test yaml_parse with valid and invalid YAML

    7. **examples/memory_example.yaml** - Memory example:
       - Show workflow with memory enabled
       - Demonstrate cross-run memory persistence
       - Show agent, workflow, and node scopes

    8. **examples/tools_example.yaml** - Tools example:
       - Show web search tool integration
       - Show file I/O tools
       - Show data processing tools
       - Demonstrate error handling with on_error: continue

    Follow integration patterns:
    - Memory repo passed via executor (like other repos)
    - Tools bound via LangChain bind_tools() pattern
    - Error handling matches existing node error pattern
  </action>
  <verify>
    Run tests: pytest tests/tools/test_data_tools.py -v
    Validate examples: python -m configurable_agents validate examples/memory_example.yaml examples/tools_example.yaml
    Run tools example: python -m configurable_agents run examples/tools_example.yaml (with SERPER_API_KEY set)
  </verify>
  <done>
    Data tools (sql, csv, json, yaml) and system tools (shell, process, env_vars) implemented. Node executor creates AgentMemory and binds tools from config. Examples demonstrate memory and tool usage.
  </done>
</task>

</tasks>

<verification>
After all tasks complete, verify:

1. **Memory persistence:**
   - Run workflow twice with memory enabled
   - Second run reads data written by first run
   - Different scopes (agent/workflow/node) properly isolate data

2. **Tool functionality:**
   - Web search returns results from configured API
   - File tools read/write within allowed directories
   - Data tools parse and transform data correctly
   - System tools execute with proper security restrictions

3. **Error handling:**
   - Tool errors fail node by default
   - on_error: continue catches errors and returns error result
   - Workflow continues after tool error in continue mode

4. **All tests pass:**
   - `pytest tests/memory/ tests/tools/ -v`
   - Coverage >80% for new code
</verification>

<success_criteria>
Phase 4 Plan 02 complete when:
1. AgentMemory provides dict-like reads and explicit writes with namespace isolation
2. 12+ tools implemented across web, file, data, system categories
3. Node executor creates memory context and binds tools from YAML config
4. Example workflows demonstrate memory and tool usage
5. All tests pass (test suite >600 lines total)
</success_criteria>

<output>
After completion, create `.planning/phases/04-advanced-capabilities/04-02-SUMMARY.md` with:
- Phase/plan metadata header
- Duration and completion date
- Tasks accomplished and commits
- Files created/modified
- Deviations from plan (if any)
- Verification results
</output>
