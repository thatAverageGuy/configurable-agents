---
phase: 04-advanced-capabilities
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - src/configurable_agents/observability/mlflow_tracker.py
  - src/configurable_agents/optimization/__init__.py
  - src/configurable_agents/optimization/ab_test.py
  - src/configurable_agents/optimization/evaluator.py
  - src/configurable_agents/optimization/gates.py
  - src/configurable_agents/config/schema.py
  - src/configurable_agents/cli.py
  - src/configurable_agents/ui/dashboard/routes/optimization.py
  - src/configurable_agents/ui/dashboard/templates/optimization.html
  - src/configurable_agents/ui/dashboard/templates/experiments.html
  - tests/optimization/test_ab_test.py
  - tests/optimization/test_evaluator.py
  - tests/optimization/test_gates.py
  - examples/mlflow_example.yaml
autonomous: true
user_setup:
  - service: mlflow
    why: "MLFlow tracking server for experiment storage and analysis"
    env_vars:
      - name: MLFLOW_TRACKING_URI
        source: "Set to 'sqlite:///mlflow.db' for local or remote MLFlow server URL"
    dashboard_config:
      - task: "Start MLFlow UI"
        location: "Run 'mlflow ui' command, visit http://localhost:5000"

must_haves:
  truths:
    - "User can define prompt variants in YAML and run A/B tests automatically"
    - "User can run workflow multiple times and MLFlow groups results by experiment"
    - "User can compare variants via CLI: gsd evaluate --experiment NAME --metric cost"
    - "User can view experiment comparison in dashboard with visual charts"
    - "Quality gates (cost, latency max) warn/fail based on on_fail config"
    - "User can apply optimized prompt from MLFlow back to YAML via CLI or UI"
  artifacts:
    - path: "src/configurable_agents/optimization/ab_test.py"
      provides: "A/B test runner for prompt variants"
      exports: ["run_ab_test", "VariantConfig", "ABTestRunner"]
      min_lines: 150
    - path: "src/configurable_agents/optimization/evaluator.py"
      provides: "Experiment evaluation and comparison"
      exports: ["ExperimentEvaluator", "compare_variants", "find_best_variant"]
      min_lines: 150
    - path: "src/configurable_agents/optimization/gates.py"
      provides: "Quality gate enforcement"
      exports: ["QualityGate", "GateAction", "check_gates"]
      min_lines: 100
    - path: "src/configurable_agents/ui/dashboard/routes/optimization.py"
      provides: "Dashboard routes for experiment comparison"
      exports: ["router", "experiments_page", "compare_page", "apply_route"]
      min_lines: 150
  key_links:
    - from: "src/configurable_agents/optimization/ab_test.py"
      to: "src/configurable_agents/observability/mlflow_tracker"
      via: "Create MLFlow runs for each variant, track metrics"
      pattern: "mlflow_tracker.log_metrics.*variant"
    - from: "src/configurable_agents/optimization/evaluator.py"
      to: "mlflow"
      via: "Query MLFlow runs for experiment comparison"
      pattern: "mlflow.search_runs.*experiment"
    - from: "src/configurable_agents/optimization/gates.py"
      to: "src/configurable_agents/runtime/executor"
      via: "Check gates after workflow execution, take action based on on_fail"
      pattern: "check_gates.*metrics"
    - from: "src/configurable_agents/cli.py"
      to: "src/configurable_agents/optimization"
      via: "CLI commands: evaluate, apply-optimized"
      pattern: "evaluate.*experiment"
    - from: "src/configurable_agents/ui/dashboard/routes/optimization.py"
      to: "src/configurable_agents/optimization"
      via: "Import evaluator and apply functions for dashboard use"
      pattern: "from configurable_agents.optimization import.*evaluator"
---

<objective>
Implement MLFlow-based prompt optimization with A/B testing, experiment comparison, quality gates, and bidirectional prompt synchronization between YAML and MLFlow.

Purpose: Enable systematic prompt experimentation (OBS-01) with automated A/B testing, visual comparison via dashboard, and quality gates to enforce cost and latency thresholds.

Output: Complete optimization system with config-driven A/B testing, CLI/dashboard evaluation interfaces, quality gate enforcement, and prompt apply workflow.
</objective>

<execution_context>
@C:\Users\ghost\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\ghost\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-advanced-capabilities/04-CONTEXT.md
@src/configurable_agents/observability/mlflow_tracker.py
@src/configurable_agents/config/schema.py
@src/configurable_agents/core/node_executor.py
@src/configurable_agents/cli.py
@src/configurable_agents/ui/dashboard/routes/workflows.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create A/B testing runner and experiment evaluator</name>
  <files>
    src/configurable_agents/optimization/__init__.py
    src/configurable_agents/optimization/ab_test.py
    src/configurable_agents/optimization/evaluator.py
    tests/optimization/test_ab_test.py
    tests/optimization/test_evaluator.py
  </files>
  <action>
    Create A/B testing and evaluation modules:

    1. **src/configurable_agents/optimization/ab_test.py** - A/B test runner:
       - `VariantConfig` dataclass: name, prompt (str), config_overrides (dict | None)
       - `ABTestConfig` dataclass: experiment_name, variants (list[VariantConfig]), run_count (int = 3)
       - `ABTestRunner` class:
         - `__init__(mlflow_tracker: MLFlowTracker)`
         - `run(config: ABTestConfig, workflow_path: str, inputs: dict) -> dict`:
           - Create MLFlow experiment if not exists
           - For each variant: run workflow N times, log metrics to MLFlow
           - Tag runs with variant name, prompt hash
           - Return summary: {variant_name: {runs, metrics: {cost, latency, tokens, success_rate}}}
         - `run_variant(workflow_path: str, variant: VariantConfig, inputs: dict, run_id: str)`:
           - Override workflow prompt with variant.prompt
           - Execute workflow via run_workflow
           - Track metrics via MLFlow tracker
       - Use existing MLFlowTracker from observability layer

    2. **src/configurable_agents/optimization/evaluator.py** - Experiment comparison:
       - `ExperimentEvaluator` class:
         - `__init__(mlflow_tracking_uri: str)`
         - `get_experiment_runs(experiment_name: str) -> list[dict]`:
           - Query MLFlow for all runs in experiment
           - Return list of {run_id, params, metrics, tags}
         - `aggregate_by_variant(runs: list[dict]) -> dict`:
           - Group runs by variant tag
           - Calculate avg, p95, p99 for cost, latency, tokens
           - Return {variant_name: {metrics: {cost: {avg, p95, p99}, ...}}}
         - `compare_variants(experiment_name: str, metric: str = "cost") -> dict`:
           - Get experiment runs and aggregate by variant
           - Sort variants by metric (lower is better for cost/latency)
           - Return comparison table with rankings
         - `find_best_variant(experiment_name: str, primary_metric: str = "cost") -> dict`:
           - Return variant with best primary_metric score
           - Include run_id for prompt retrieval
         - `get_prompt_from_run(run_id: str) -> str`:
           - Retrieve original prompt from MLFlow run params
       - Helper functions:
         - `format_comparison_table(comparison: dict) -> str` for CLI output
         - `calculate_percentiles(values: list[float]) -> dict` for p95/p99

    3. **src/configurable_agents/optimization/__init__.py** - Public API:
       - Export `ABTestRunner`, `ABTestConfig`, `VariantConfig`
       - Export `ExperimentEvaluator`, `compare_variants`, `find_best_variant`
       - Export `format_comparison_table`

    4. **tests/optimization/test_ab_test.py** - Test suite (min 150 lines):
       - Mock MLFlow tracker for unit tests
       - Test variant config creation
       - Test A/B test runner with mock workflow execution
       - Test metric aggregation
       - Test run tagging with variant names
       - Use existing mock patterns from observability tests

    5. **tests/optimization/test_evaluator.py** - Test suite (min 150 lines):
       - Mock MLFlow client for query operations
       - Test experiment run retrieval
       - Test variant aggregation
       - Test comparison sorting
       - Test best variant selection
       - Test prompt retrieval from run params

    Follow MLFlow patterns:
    - Use MLFlow client API for queries
    - Tag runs for variant identification
    - Log params for prompt storage
    - Calculate percentiles using numpy or pure Python
  </action>
  <verify>
    Run tests: pytest tests/optimization/test_ab_test.py tests/optimization/test_evaluator.py -v
    Verify imports: python -c "from configurable_agents.optimization import ABTestRunner, ExperimentEvaluator"
  </verify>
  <done>
    ABTestRunner executes workflow variants and tracks metrics to MLFlow. ExperimentEvaluator queries MLFlow, aggregates metrics by variant, and finds best performing prompt.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create quality gates and integrate A/B testing into config schema</name>
  <files>
    src/configurable_agents/optimization/gates.py
    src/configurable_agents/config/schema.py
    src/configurable_agents/core/node_executor.py
    src/configurable_agents/runtime/executor.py
    tests/optimization/test_gates.py
  </files>
  <action>
    Create quality gates and integrate optimization into workflow system:

    1. **src/configurable_agents/optimization/gates.py** - Quality gate enforcement:
       - `GateAction` enum: WARN, FAIL, BLOCK_DEPLOY
       - `QualityGate` dataclass: metric (str), max (float), min (float | None = None)
       - `GatesConfig` dataclass: gates (list[QualityGate]), on_fail (GateAction = GateAction.WARN)
       - `check_gates(metrics: dict, config: GatesConfig) -> list[dict]`:
         - For each gate: check if metric exceeds max or below min
         - Return list of {gate, passed, actual, threshold}
       - `take_action(results: list[dict], action: GateAction) -> None`:
         - WARN: log warnings, don't raise
         - FAIL: raise QualityGateError with failed gates
         - BLOCK_DEPLOY: set flag in storage, log error
       - `QualityGateError` exception class

    2. **src/configurable_agents/config/schema.py** - Extend schema for optimization:
       - Add `MLFlowConfig` model:
         - `experiment: str | None = None` (workflow-level experiment name)
         - `run_name: str | None = None` (optional run identifier)
       - Add `VariantConfig` model (for YAML variants):
         - `name: str`
         - `prompt: str`
       - Add `ABTestConfig` model:
         - `enabled: bool = False`
         - `experiment: str`
         - `variants: list[VariantConfig]`
         - `run_count: int = 3`
         - `parallel: bool = True` (run variants concurrently)
       - Add `GatesConfig` model (reuse from gates module)
       - Add `mlflow: MLFlowConfig | None = None` to `WorkflowConfig`
       - Add `mlflow: MLFlowConfig | None = None` to `NodeConfig` (override workflow default)
       - Add `ab_test: ABTestConfig | None = None` to `WorkflowConfig`
       - Add `gates: GatesConfig | None = None` to `WorkflowConfig`

    3. **src/configurable_agents/runtime/executor.py** - Wire up optimization:
       - Import gates: `from configurable_agents.optimization.gates import check_gates, take_action, GateAction`
       - Import A/B test runner: `from configurable_agents.optimization.ab_test import ABTestRunner`
       - In `run_workflow()`, check for ab_test config:
         - If `config.ab_test.enabled`:
           - Create ABTestRunner with MLFlow tracker
           - Run all variants with configured run_count
           - Output comparison table to console
           - Return aggregated results
       - After workflow execution, check quality gates:
         - If `config.gates`:
           - Collect metrics from execution (cost, latency, tokens)
           - Call `check_gates(metrics, config.gates)`
           - Call `take_action(results, config.gates.on_fail)`
         - Log gate results at appropriate level

    4. **tests/optimization/test_gates.py** - Test suite (min 100 lines):
       - Test gate checking for various metrics
       - Test WARN action (no exception, logs warning)
       - Test FAIL action (raises QualityGateError)
       - Test BLOCK_DEPLOY action (sets flag)
       - Test min/max threshold variants
       - Test multiple gates evaluation

    Follow existing executor patterns:
    - Check ab_test before normal workflow execution
    - Check gates after workflow completes (success or failure)
    - Use existing logger for output
    - Return structured results from ab_test mode
  </action>
  <verify>
    Run tests: pytest tests/optimization/test_gates.py -v
    Validate config with gates: python -m configurable_agents validate examples/mlflow_example.yaml
  </verify>
  <done>
    Quality gates check metrics against thresholds and take configured action. Executor runs A/B tests and checks gates when configured in YAML.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add CLI commands and dashboard UI for optimization</name>
  <files>
    src/configurable_agents/cli.py
    src/configurable_agents/ui/dashboard/routes/optimization.py
    src/configurable_agents/ui/dashboard/templates/optimization.html
    src/configurable_agents/ui/dashboard/templates/experiments.html
    src/configurable_agents/ui/dashboard/app.py
    tests/cli/test_optimization_commands.py
    examples/mlflow_example.yaml
  </files>
  <action>
    Add CLI and dashboard interfaces for optimization:

    1. **src/configurable_agents/cli.py** - Add optimization commands:
       - Add `evaluate` command:
         - `--experiment NAME`: Experiment to evaluate
         - `--metric METRIC`: Metric for comparison (default: cost)
         - `--mlflow-uri URI`: MLFlow tracking URI
         - Calls `ExperimentEvaluator.compare_variants()`
         - Prints Rich table with variant comparison
         - Highlights best variant in green
       - Add `apply-optimized` command:
         - `--experiment NAME`: Experiment with best variant
         - `--variant VARIANT`: Specific variant to apply (optional, defaults to best)
         - `--workflow PATH`: Workflow file to update
         - `--dry-run`: Show diff without applying
         - Calls `find_best_variant()` and updates YAML
         - Shows prompt diff before applying
       - Add `ab-test` command:
         - `workflow.yaml`: Workflow config with variants
         - Runs A/B test using ABTestRunner
         - Displays progress and results
       - Add `optimization` command group:
         - Subcommands: evaluate, apply-optimized, ab-test

    2. **src/configurable_agents/optimization/ab_test.py** - Add apply function:
       - Add `apply_prompt_to_workflow(workflow_path: str, prompt: str, node_id: str | None = None) -> None`:
         - Load workflow config
         - Update prompt (workflow default or specific node)
         - Write updated config
         - Create backup of original file

    3. **src/configurable_agents/ui/dashboard/routes/optimization.py** - Dashboard routes:
       - `GET /optimization/experiments` - Experiments list page
       - `GET /optimization/experiment/{name}` - Experiment detail page
       - `GET /optimization/compare` - Comparison page (accepts experiment, metric params)
       - `POST /optimization/apply` - Apply optimized prompt
       - Use HTMX for partial table updates
       - Embed MLFlow iframe for charts
       - Render templates with experiment data

    4. **src/configurable_agents/ui/dashboard/templates/experiments.html** - Experiments page:
       - Table of all MLFlow experiments
       - Links to detail/comparison pages
       - HTMX auto-refresh every 10s

    5. **src/configurable_agents/ui/dashboard/templates/optimization.html** - Comparison page:
       - Variant comparison table (same data as CLI)
       - Chart visualizations (cost, latency bar charts)
       - "Apply Best" button with confirmation
       - Diff view for prompt comparison
       - Use existing dashboard CSS

    6. **src/configurable_agents/ui/dashboard/app.py** - Wire up optimization routes:
       - Import optimization router
       - Include router in app
       - Pass mlflow_tracker via app.state

    7. **tests/cli/test_optimization_commands.py** - CLI test suite (min 150 lines):
       - Test evaluate command with mock MLFlow data
       - Test apply-optimized with dry-run
       - Test ab-test command execution
       - Test optimization command group
       - Mock file operations for apply tests

    8. **examples/mlflow_example.yaml** - MLFlow optimization example:
       - Show mlflow config at workflow and node level
       - Show A/B test variants definition
       - Show quality gates configuration
       - Include README explaining usage

    Follow existing CLI/dashboard patterns:
    - Rich table formatting matches observability commands
    - HTMX patterns match existing dashboard routes
    - Lazy MLFlow import for help without MLFlow installed
    - Repository injection via app.state
  </action>
  <verify>
    Run CLI tests: pytest tests/cli/test_optimization_commands.py -v
    Test CLI help: python -m configurable_agents optimization --help
    Test dashboard: Start dashboard, visit /optimization/experiments
  </verify>
  <done>
    CLI provides evaluate, apply-optimized, and ab-test commands with Rich output. Dashboard shows experiments, comparison tables, and apply button. Example demonstrates optimization workflow.
  </done>
</task>

</tasks>

<verification>
After all tasks complete, verify:

1. **A/B testing workflow:**
   - Create workflow with A/B test variants in YAML
   - Run `configurable-agents ab-test workflow.yaml`
   - Verify all variants executed and metrics tracked in MLFlow
   - MLFlow UI shows runs grouped by variant

2. **CLI evaluation:**
   - Run `configurable-agents evaluate --experiment NAME`
   - See Rich table with variant comparison
   - Best variant highlighted

3. **Dashboard UI:**
   - Visit /optimization/experiments - see experiment list
   - Click experiment - see comparison with charts
   - Click "Apply Best" - prompt updated in workflow file

4. **Quality gates:**
   - Run workflow with gates configured
   - Exceed threshold - see warning/error based on on_fail
   - BLOCK_DEPLOY flag prevents further actions

5. **All tests pass:**
   - `pytest tests/optimization/ tests/cli/test_optimization_commands.py -v`
   - Coverage >80% for new code
</verification>

<success_criteria>
Phase 4 Plan 03 complete when:
1. A/B test runner executes variants and tracks metrics to MLFlow
2. Experiment evaluator compares variants and finds best performing
3. Quality gates check metrics and take configured action
4. CLI provides evaluate, apply-optimized, ab-test commands
5. Dashboard shows experiments, comparison tables, and charts
6. Example demonstrates A/B testing and quality gates
7. All tests pass (test suite >650 lines total)
</success_criteria>

<output>
After completion, create `.planning/phases/04-advanced-capabilities/04-03-SUMMARY.md` with:
- Phase/plan metadata header
- Duration and completion date
- Tasks accomplished and commits
- Files created/modified
- Deviations from plan (if any)
- Verification results

Also update `.planning/ROADMAP.md` to mark Phase 4 as complete with all 3 plans.
</output>
