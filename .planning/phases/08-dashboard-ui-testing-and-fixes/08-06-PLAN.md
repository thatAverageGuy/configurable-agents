---
phase: 08-dashboard-ui-testing-and-fixes
plan: 06
type: execute
wave: 3
depends_on: [08-01, 08-02, 08-03, 08-04]
files_modified:
  - tests/ui/test_dashboard_integration.py
autonomous: true

must_haves:
  truths:
    - "Template rendering verified for all templates (no syntax errors)"
    - "Error handling verified (404, 500, MLFlow unavailable)"
    - "Route parameters tested (status filters, experiment names)"
    - "Form submission endpoints tested"
  artifacts:
    - path: "tests/ui/test_dashboard_integration.py"
      provides: "Integration tests for template rendering and error handling"
      min_lines: 150
      contains: "TestTemplateRendering, TestErrorHandling, TestRouteParameters"
  key_links:
    - from: "tests/ui/test_dashboard_integration.py"
      to: "All dashboard templates"
      via: "Direct template rendering tests"
      pattern: "TemplateResponse|templates\\.TemplateResponse"
    - from: "tests/ui/test_dashboard_integration.py"
      to: "error handlers"
      via: "Test error responses and friendly error pages"
      pattern: "test.*error|test.*404|test.*500"
---

<objective>
Create integration tests for template rendering, error handling, and route parameters to ensure comprehensive coverage.

Purpose: E2E tests (08-05) cover happy path page loads. Need additional tests for:
- Template rendering (verify all templates compile without errors)
- Error handling (404, 500, MLFlow unavailable scenarios)
- Route parameters (status filters, query params)
- Form submission endpoints

Output: Integration tests covering edge cases and error scenarios.
</objective>

<execution_context>
@C:\Users\ghost\.claude\get-shit-done\workflows\execute-plan.md
@C:\Users\ghost\.claude\get-shit-done\templates\summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/08-dashboard-ui-testing-and-fixes/08-CONTEXT.md

# Existing test patterns
@tests/ui/test_dashboard.py
@tests/conftest.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Create test_dashboard_integration.py with template and error tests</name>
  <files>tests/ui/test_dashboard_integration.py</files>
  <action>
    Create new file with following test classes:

    **Class TestTemplateRendering:**
    - `test_base_template_renders` - Verify base.html compiles without errors
    - `test_dashboard_template_renders` - Verify dashboard.html extends base
    - `test_workflows_template_renders` - Verify workflows.html compiles
    - `test_agents_template_renders` - Verify agents.html compiles
    - `test_experiments_template_renders` - Verify experiments.html compiles
    - `test_mlflow_unavailable_template_renders` - Verify friendly error template
    - `test_macros_template_renders` - Verify macros.html is valid Jinja2

    **Class TestErrorHandling:**
    - `test_workflow_not_found_returns_error` - GET /workflows/{nonexistent} returns error
    - `test_agent_not_found_returns_404` - DELETE /agents/{nonexistent} returns 404
    - `test_mlflow_unavailable_returns_html` - /mlflow without MLFlow mounted returns HTML
    - `test_optimization_without_mlflow_shows_message` - /optimization/compare with bad exp shows error

    **Class TestRouteParameters:**
    - `test_workflows_status_filter` - GET /workflows?status=running filters correctly
    - `test_workflows_table_status_filter` - GET /workflows/table?status=completed
    - `test_experiments_metric_param` - GET /optimization/experiments with metric param
    - `test_optimization_compare_params` - GET /optimization/compare with experiment and metric

    **Class TestFormSubmission:**
    - `test_workflow_cancel_running` - POST /workflows/{id}/cancel
    - `test_agents_delete_valid` - DELETE /agents/{id} for valid agent
    - `test_agents_refresh` - POST /agents/refresh returns table

    Use httpx.AsyncClient with ASGITransport pattern.
    Seed test data with in-memory SQLite for realistic testing.
  </action>
  <verify>
    test_dashboard_integration.py exists with tests for templates, errors, params, forms
  </verify>
  <done>
    Integration tests cover templates, errors, route parameters, and form submissions
  </done>
</task>

</tasks>

<verification>
1. Run integration tests: `pytest tests/ui/test_dashboard_integration.py -v`
2. Verify all template rendering tests pass
3. Verify error handling tests work
4. Verify route parameter tests pass
5. Verify form submission tests work
</verification>

<success_criteria>
- All dashboard templates verified to compile
- Error scenarios tested (404, MLFlow unavailable)
- Route parameters tested
- Form submission endpoints tested
- Tests pass with fixes from 08-01 through 08-04
</success_criteria>

<output>
After completion, create `.planning/phases/08-dashboard-ui-testing-and-fixes/08-06-SUMMARY.md`
</output>
