---
phase: 01-core-engine
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/configurable_agents/storage/__init__.py
  - src/configurable_agents/storage/base.py
  - src/configurable_agents/storage/models.py
  - src/configurable_agents/storage/sqlite.py
  - src/configurable_agents/storage/factory.py
  - src/configurable_agents/config/schema.py
  - tests/storage/__init__.py
  - tests/storage/test_base.py
  - tests/storage/test_sqlite.py
  - tests/storage/test_factory.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "Storage interface defines add/get/list/update/delete for workflow runs and execution state"
    - "SQLite implementation persists workflow run records to a .db file"
    - "Factory function creates the correct backend from a config string like 'sqlite:///workflows.db'"
    - "All storage operations are tested without external dependencies"
  artifacts:
    - path: "src/configurable_agents/storage/base.py"
      provides: "Abstract repository interfaces (AbstractWorkflowRunRepository, AbstractExecutionStateRepository)"
      exports: ["AbstractWorkflowRunRepository", "AbstractExecutionStateRepository"]
    - path: "src/configurable_agents/storage/models.py"
      provides: "SQLAlchemy ORM models for workflow_runs and execution_state tables"
      contains: "class WorkflowRunRecord"
    - path: "src/configurable_agents/storage/sqlite.py"
      provides: "SQLite implementation of both repository interfaces"
      exports: ["SQLiteWorkflowRunRepository", "SQLiteExecutionStateRepository"]
    - path: "src/configurable_agents/storage/factory.py"
      provides: "create_storage_backend factory function"
      exports: ["create_storage_backend"]
  key_links:
    - from: "src/configurable_agents/storage/sqlite.py"
      to: "src/configurable_agents/storage/base.py"
      via: "implements abstract interfaces"
      pattern: "class SQLiteWorkflowRunRepository\\(AbstractWorkflowRunRepository\\)"
    - from: "src/configurable_agents/storage/factory.py"
      to: "src/configurable_agents/storage/sqlite.py"
      via: "factory instantiation"
      pattern: "SQLiteWorkflowRunRepository"
---

<objective>
Create a pluggable storage abstraction layer with SQLite as the default implementation, enabling workflow run persistence and execution state tracking.

Purpose: ARCH-04 requires storage backend to be pluggable (SQLite default, swappable to PostgreSQL/Redis without code changes). This is foundational infrastructure that all later phases depend on (agent registry in Phase 2, session persistence in Phase 3, long-term memory in Phase 4).

Output: A complete `src/configurable_agents/storage/` module with abstract interfaces, SQLAlchemy ORM models, SQLite implementation, and factory function. All tested.
</objective>

<execution_context>
@C:\Users\ghost\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\ghost\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-engine/01-RESEARCH.md
@src/configurable_agents/config/schema.py
@src/configurable_agents/observability/cost_estimator.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create storage abstraction interfaces and SQLAlchemy ORM models</name>
  <files>
    src/configurable_agents/storage/__init__.py
    src/configurable_agents/storage/base.py
    src/configurable_agents/storage/models.py
    pyproject.toml
  </files>
  <action>
1. Add `sqlalchemy>=2.0.46` to dependencies in pyproject.toml (alongside existing deps). Do NOT add aiosqlite or asyncpg yet -- synchronous-only for v1.

2. Create `src/configurable_agents/storage/base.py` with two abstract repository interfaces using ABC:

   **AbstractWorkflowRunRepository** with methods:
   - `add(run: WorkflowRunRecord) -> None` -- persist a new run
   - `get(run_id: str) -> Optional[WorkflowRunRecord]` -- get by ID
   - `list_by_workflow(workflow_name: str, limit: int = 100) -> List[WorkflowRunRecord]` -- list runs for a workflow
   - `update_status(run_id: str, status: str) -> None` -- update run status
   - `delete(run_id: str) -> None` -- delete a run

   **AbstractExecutionStateRepository** with methods:
   - `save_state(run_id: str, state_data: dict, node_id: str) -> None` -- save state checkpoint after a node
   - `get_latest_state(run_id: str) -> Optional[dict]` -- get latest state for a run
   - `get_state_history(run_id: str) -> List[dict]` -- get all state checkpoints for a run

   Use `from abc import ABC, abstractmethod`. Follow existing project conventions (type annotations required, Google-style docstrings).

3. Create `src/configurable_agents/storage/models.py` with SQLAlchemy 2.0 ORM models:

   **Base class:**
   ```python
   from sqlalchemy.orm import DeclarativeBase
   class Base(DeclarativeBase):
       pass
   ```

   **WorkflowRunRecord** (table: `workflow_runs`):
   - `id: Mapped[str]` -- UUID primary key (String(36))
   - `workflow_name: Mapped[str]` -- workflow name (String(256))
   - `status: Mapped[str]` -- status enum string: "pending", "running", "completed", "failed" (String(32))
   - `config_snapshot: Mapped[Optional[str]]` -- JSON serialized config (Text, nullable)
   - `inputs: Mapped[Optional[str]]` -- JSON serialized inputs (Text, nullable)
   - `outputs: Mapped[Optional[str]]` -- JSON serialized outputs (Text, nullable)
   - `error_message: Mapped[Optional[str]]` -- error message if failed (Text, nullable)
   - `started_at: Mapped[datetime]` -- when the run started (DateTime, default=utcnow)
   - `completed_at: Mapped[Optional[datetime]]` -- when it completed (DateTime, nullable)
   - `duration_seconds: Mapped[Optional[float]]` -- execution time (Float, nullable)
   - `total_tokens: Mapped[Optional[int]]` -- total tokens used (Integer, nullable)
   - `total_cost_usd: Mapped[Optional[float]]` -- total cost (Float, nullable)

   Use SQLAlchemy 2.0 `Mapped` and `mapped_column` syntax (NOT legacy Column syntax). Import `from sqlalchemy import String, Text, Float, Integer, DateTime` and `from sqlalchemy.orm import Mapped, mapped_column`.

   **ExecutionStateRecord** (table: `execution_states`):
   - `id: Mapped[int]` -- auto-increment primary key (Integer)
   - `run_id: Mapped[str]` -- foreign key to workflow_runs.id (String(36), ForeignKey)
   - `node_id: Mapped[str]` -- which node produced this state (String(128))
   - `state_data: Mapped[str]` -- JSON serialized state (Text)
   - `created_at: Mapped[datetime]` -- timestamp (DateTime, default=utcnow)

4. Create `src/configurable_agents/storage/__init__.py` that exports the public API:
   - From base: AbstractWorkflowRunRepository, AbstractExecutionStateRepository
   - From models: WorkflowRunRecord (the ORM model), ExecutionStateRecord, Base
   - From factory (Task 2): create_storage_backend

5. Add `StorageConfig` to `src/configurable_agents/config/schema.py` inside the GlobalConfig section:

   ```python
   class StorageConfig(BaseModel):
       """Storage backend configuration."""
       backend: str = Field("sqlite", description="Storage backend type: 'sqlite' or connection URI")
       path: str = Field("./workflows.db", description="SQLite database path (only for sqlite backend)")
   ```

   Add `storage: Optional[StorageConfig] = Field(None, description="Storage backend config")` to the `GlobalConfig` model. Export `StorageConfig` from `config/__init__.py`.

IMPORTANT: Use SQLAlchemy 2.0 patterns throughout. Use `with Session(engine) as session:` context managers (NOT `session = Session(engine)` without context manager). See research pitfall #3 about transaction auto-start.
  </action>
  <verify>
Run `python -c "from configurable_agents.storage.base import AbstractWorkflowRunRepository, AbstractExecutionStateRepository; from configurable_agents.storage.models import Base, WorkflowRunRecord, ExecutionStateRecord; print('imports OK')"` to confirm modules load.

Run `python -c "from configurable_agents.config.schema import StorageConfig, GlobalConfig; gc = GlobalConfig(storage={'backend': 'sqlite', 'path': 'test.db'}); print(gc.storage.backend)"` to confirm config schema works.
  </verify>
  <done>
Abstract interfaces defined with all CRUD methods. SQLAlchemy 2.0 ORM models created with correct Mapped syntax. StorageConfig added to GlobalConfig. All imports resolve cleanly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement SQLite repository and factory with tests</name>
  <files>
    src/configurable_agents/storage/sqlite.py
    src/configurable_agents/storage/factory.py
    src/configurable_agents/storage/__init__.py
    tests/storage/__init__.py
    tests/storage/test_base.py
    tests/storage/test_sqlite.py
    tests/storage/test_factory.py
  </files>
  <action>
1. Create `src/configurable_agents/storage/sqlite.py` implementing both abstract interfaces:

   **SQLiteWorkflowRunRepository(AbstractWorkflowRunRepository):**
   - Constructor takes `engine: Engine` parameter (NOT db_path -- engine is created by factory)
   - `add(run)`: Use `with Session(self.engine) as session: session.add(run); session.commit()`
   - `get(run_id)`: Use `session.get(WorkflowRunRecord, run_id)`
   - `list_by_workflow(workflow_name, limit)`: Query with filter, order by `started_at DESC`, limit
   - `update_status(run_id, status)`: Get record, update status field, commit. Also update `completed_at` if status is "completed" or "failed"
   - `delete(run_id)`: Get record, delete, commit

   **SQLiteExecutionStateRepository(AbstractExecutionStateRepository):**
   - Constructor takes `engine: Engine`
   - `save_state(run_id, state_data, node_id)`: Create ExecutionStateRecord with `json.dumps(state_data)`, add and commit
   - `get_latest_state(run_id)`: Query ExecutionStateRecord filtered by run_id, order by created_at DESC, limit 1, return `json.loads(record.state_data)`
   - `get_state_history(run_id)`: Query all records for run_id, order by created_at ASC, return list of dicts with node_id, state_data (parsed), created_at

   IMPORTANT: Every database operation MUST use `with Session(self.engine) as session:` context manager pattern. DO NOT use bare session creation.

2. Create `src/configurable_agents/storage/factory.py`:

   ```python
   def create_storage_backend(config: Optional[StorageConfig] = None) -> tuple[SQLiteWorkflowRunRepository, SQLiteExecutionStateRepository]:
   ```

   - If config is None, use defaults (sqlite, ./workflows.db)
   - If backend is "sqlite" or starts with "sqlite:///", create SQLAlchemy engine: `create_engine(f"sqlite:///{config.path}")`
   - Call `Base.metadata.create_all(engine)` to ensure tables exist
   - Return tuple of (SQLiteWorkflowRunRepository(engine), SQLiteExecutionStateRepository(engine))
   - For any other backend string, raise `ValueError(f"Unsupported storage backend: {config.backend}. Supported: 'sqlite'. PostgreSQL coming in Phase 2.")`

   Update `storage/__init__.py` to export `create_storage_backend`.

3. Create tests:

   **tests/storage/__init__.py**: Empty file.

   **tests/storage/test_base.py**: Test that abstract classes cannot be instantiated directly (`pytest.raises(TypeError)`). Test that subclasses must implement all methods.

   **tests/storage/test_sqlite.py**: Use `tmp_path` fixture for temp database files. Test:
   - Add a workflow run, get it back, verify fields match
   - List runs by workflow name (add 3 runs, verify list returns them ordered by started_at DESC)
   - Update status to "completed", verify completed_at is set
   - Update status to "failed", verify completed_at is set
   - Delete a run, verify get returns None
   - Save execution state, get latest state, verify round-trip through JSON
   - Save multiple states, get history, verify order and completeness
   - Get latest state for non-existent run returns None

   **tests/storage/test_factory.py**: Test:
   - Default config creates SQLite repos with tmp_path
   - Explicit sqlite config creates repos
   - Unsupported backend raises ValueError
   - Tables are created automatically (verify by running a query)

   Follow existing test conventions: class-based grouping (TestSQLiteWorkflowRunRepo, TestSQLiteExecutionStateRepo, TestFactory), Google-style docstrings, pytest fixtures for engine/repos.
  </action>
  <verify>
Run `pytest tests/storage/ -v` and confirm all tests pass.
Run `pytest tests/ -v --tb=short` to confirm existing tests still pass (no regressions).
  </verify>
  <done>
SQLite repository implements all CRUD operations. Factory creates backend from config. All new tests pass. All existing 645+ tests still pass with no regressions.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from configurable_agents.storage import create_storage_backend; repos = create_storage_backend(); print(type(repos[0]).__name__)"` prints "SQLiteWorkflowRunRepository"
2. `pytest tests/storage/ -v` -- all storage tests pass
3. `pytest tests/ -v --tb=short` -- full test suite passes (no regressions)
4. Storage module follows project conventions: type annotations, Google-style docstrings, module-level loggers
</verification>

<success_criteria>
- Abstract interfaces defined with complete CRUD operations for workflow runs and execution state
- SQLite implementation persists records to disk via SQLAlchemy 2.0
- Factory creates correct backend from StorageConfig
- StorageConfig added to GlobalConfig schema
- All tests pass including existing test suite (zero regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-engine/01-01-SUMMARY.md`
</output>
