---
phase: 01-core-engine
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/configurable_agents/llm/provider.py
  - src/configurable_agents/llm/litellm_provider.py
  - src/configurable_agents/llm/__init__.py
  - src/configurable_agents/config/schema.py
  - src/configurable_agents/config/validator.py
  - src/configurable_agents/config/__init__.py
  - src/configurable_agents/runtime/feature_gate.py
  - src/configurable_agents/observability/cost_estimator.py
  - tests/llm/__init__.py
  - tests/llm/test_litellm_provider.py
  - tests/llm/test_provider.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "User can set provider to 'openai', 'anthropic', 'google', or 'ollama' in YAML config and the workflow executes with that provider"
    - "User can run a workflow with ollama_chat/llama3 model and no internet connection (local model)"
    - "Changing one line in YAML (provider/model) switches the LLM provider without any code changes"
    - "Cost tracking works across all providers via LiteLLM's built-in cost calculation"
  artifacts:
    - path: "src/configurable_agents/llm/litellm_provider.py"
      provides: "LiteLLM-based unified provider implementation"
      exports: ["create_litellm_llm", "call_litellm_completion"]
    - path: "src/configurable_agents/llm/provider.py"
      provides: "Updated factory routing to LiteLLM for all providers"
      exports: ["create_llm", "call_llm_structured"]
    - path: "src/configurable_agents/observability/cost_estimator.py"
      provides: "Updated cost estimation using LiteLLM cost_per_token"
      contains: "litellm.completion_cost"
  key_links:
    - from: "src/configurable_agents/llm/provider.py"
      to: "src/configurable_agents/llm/litellm_provider.py"
      via: "factory routing for all providers"
      pattern: "create_litellm_llm"
    - from: "src/configurable_agents/llm/litellm_provider.py"
      to: "litellm"
      via: "litellm.completion() calls"
      pattern: "litellm\\.completion"
    - from: "src/configurable_agents/config/schema.py"
      to: "src/configurable_agents/llm/provider.py"
      via: "LLMConfig.provider field validation"
      pattern: "provider.*openai|anthropic|google|ollama"
---

<objective>
Replace the single-provider Google Gemini LLM integration with a unified multi-provider system using LiteLLM, supporting OpenAI, Anthropic, Google Gemini, and Ollama local models.

Purpose: RT-05 (multi-LLM provider support) and RT-06 (local model support via Ollama) are core Phase 1 requirements. LiteLLM provides a unified OpenAI-compatible interface for 100+ providers with built-in cost tracking, eliminating the need for provider-specific wrapper code. This is a decision already made in STATE.md.

Output: Updated `src/configurable_agents/llm/` module that routes all LLM calls through LiteLLM, with updated config validation, cost estimation, and feature gate. Google-specific module retained as fallback.
</objective>

<execution_context>
@C:\Users\ghost\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\ghost\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-engine/01-RESEARCH.md
@src/configurable_agents/llm/provider.py
@src/configurable_agents/llm/google.py
@src/configurable_agents/llm/__init__.py
@src/configurable_agents/config/schema.py
@src/configurable_agents/config/validator.py
@src/configurable_agents/observability/cost_estimator.py
@src/configurable_agents/runtime/feature_gate.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create LiteLLM provider and update factory routing</name>
  <files>
    src/configurable_agents/llm/litellm_provider.py
    src/configurable_agents/llm/provider.py
    src/configurable_agents/llm/__init__.py
    src/configurable_agents/config/schema.py
    src/configurable_agents/config/validator.py
    src/configurable_agents/config/__init__.py
    src/configurable_agents/runtime/feature_gate.py
    pyproject.toml
  </files>
  <action>
1. Add `litellm>=1.80.0` to dependencies in pyproject.toml. Keep `langchain-google-genai>=1.0.0` for backward compatibility but it will no longer be the primary path.

2. Create `src/configurable_agents/llm/litellm_provider.py`:

   **Provider model string mapping function:**
   ```python
   def get_litellm_model_string(provider: str, model: str) -> str:
   ```
   Maps our config to LiteLLM's model string format:
   - provider="openai", model="gpt-4o" -> "openai/gpt-4o"
   - provider="anthropic", model="claude-sonnet-4-20250514" -> "anthropic/claude-sonnet-4-20250514"
   - provider="google", model="gemini-2.5-flash" -> "gemini/gemini-2.5-flash" (LiteLLM uses "gemini/" not "google/")
   - provider="ollama", model="llama3" -> "ollama_chat/llama3" (use ollama_chat/ NOT ollama/ per research pitfall #4)

   **LiteLLM wrapper that returns LangChain-compatible interface:**
   ```python
   def create_litellm_llm(llm_config: LLMConfig) -> BaseChatModel:
   ```
   - Import `from langchain_community.chat_models import ChatLiteLLM` (LangChain's LiteLLM wrapper provides BaseChatModel interface)
   - Build model string using get_litellm_model_string()
   - Create ChatLiteLLM instance with model=model_string, temperature, max_tokens
   - For Ollama: set `api_base="http://localhost:11434"` unless overridden
   - Return the ChatLiteLLM instance

   IMPORTANT: ChatLiteLLM from langchain-community wraps litellm to provide the BaseChatModel interface that the rest of the codebase expects (call_llm_structured uses llm.with_structured_output). This is the correct approach -- do NOT bypass LangChain's interface.

   **Standalone completion function for direct use:**
   ```python
   def call_litellm_completion(provider: str, model: str, messages: list, **kwargs) -> dict:
   ```
   - Uses `litellm.completion()` directly
   - Returns dict with: content, input_tokens, output_tokens, total_tokens, cost_usd (via `litellm.completion_cost(response)`)
   - Handles LiteLLM-specific errors (import and catch `litellm.exceptions.APIError`, `litellm.exceptions.AuthenticationError`)

   Add `LITELLM_AVAILABLE` check at module top (try/except import like mlflow_tracker.py does).

3. Update `src/configurable_agents/llm/provider.py`:

   **Update `create_llm` function:**
   - Change `supported_providers` from `["google"]` to `["openai", "anthropic", "google", "ollama"]`
   - Add LiteLLM import check: `try: from configurable_agents.llm.litellm_provider import create_litellm_llm, LITELLM_AVAILABLE`
   - Route ALL providers through LiteLLM when available: `if LITELLM_AVAILABLE: return create_litellm_llm(llm_config)`
   - Keep Google fallback: `if provider == "google" and not LITELLM_AVAILABLE: from configurable_agents.llm.google import create_google_llm; return create_google_llm(llm_config)`
   - If LiteLLM not available and provider is not "google", raise LLMProviderError with suggestion to `pip install litellm`
   - Update the LLMProviderError message to remove "v0.1 only supports Google Gemini" text

   **Update `call_llm_structured` function:**
   - No changes needed -- it uses BaseChatModel.with_structured_output() which works with ChatLiteLLM

4. Update `src/configurable_agents/config/schema.py`:

   **LLMConfig model:**
   - Update `provider` field description from `"LLM provider (v0.1: only 'google')"` to `"LLM provider: 'openai', 'anthropic', 'google', 'ollama'"`
   - Add a field_validator for provider that validates against supported values:
     ```python
     @field_validator("provider")
     @classmethod
     def validate_provider(cls, v):
         if v is not None:
             supported = ["openai", "anthropic", "google", "ollama"]
             if v not in supported:
                 raise ValueError(f"Provider '{v}' not supported. Supported: {supported}")
         return v
     ```
   - Add optional field `api_base: Optional[str] = Field(None, description="Custom API base URL (e.g., for Ollama: http://localhost:11434)")` to LLMConfig

5. Update `src/configurable_agents/config/validator.py`:
   - If there is any provider-specific validation that blocks non-Google providers, remove that restriction

6. Update `src/configurable_agents/runtime/feature_gate.py`:
   - Remove conditional routing from `_check_conditional_routing` -- wait, no, that is about edge routing not LLM providers
   - Update `get_supported_features()`: Move "Multiple LLM providers (OpenAI, Anthropic, Ollama)" from `not_supported.v0.2` to `llm` list
   - Update `llm` feature list from `["Google Gemini models", ...]` to include all four providers
   - Remove "v0.2" note from the check_feature_support timeline for multi-provider

7. Update `src/configurable_agents/llm/__init__.py` to also export `LITELLM_AVAILABLE` (conditional), and update module docstring to mention multi-provider support.

8. Export `StorageConfig` from `config/__init__.py` if not already done. (May have been done by Plan 01 if it ran first, but do it defensively.)
  </action>
  <verify>
Run: `python -c "from configurable_agents.llm.litellm_provider import create_litellm_llm, LITELLM_AVAILABLE; print(f'LiteLLM available: {LITELLM_AVAILABLE}')"` -- should print True.

Run: `python -c "from configurable_agents.llm import create_llm; from configurable_agents.config import LLMConfig; print('factory OK')"` -- should not error.

Run: `python -c "from configurable_agents.config import LLMConfig; c = LLMConfig(provider='openai', model='gpt-4o'); print(c.provider)"` -- should print "openai".

Run: `python -c "from configurable_agents.config import LLMConfig; LLMConfig(provider='invalid')"` -- should raise ValueError.
  </verify>
  <done>
LiteLLM provider module created. Factory routes all four providers through LiteLLM. Config schema validates provider values. Feature gate updated to reflect multi-provider support. Google fallback preserved when LiteLLM unavailable.
  </done>
</task>

<task type="auto">
  <name>Task 2: Update cost estimation and write comprehensive tests</name>
  <files>
    src/configurable_agents/observability/cost_estimator.py
    tests/llm/__init__.py
    tests/llm/test_litellm_provider.py
    tests/llm/test_provider.py
  </files>
  <action>
1. Update `src/configurable_agents/observability/cost_estimator.py`:

   - Add LiteLLM cost lookup as primary cost source:
     ```python
     try:
         import litellm
         LITELLM_AVAILABLE = True
     except ImportError:
         LITELLM_AVAILABLE = False
     ```
   - In `estimate_cost()` method (or equivalent), when LITELLM_AVAILABLE:
     - Try `litellm.cost_per_token(model=model, call_type="completion")` to get per-token pricing
     - Calculate: `cost = (input_tokens * input_cost) + (output_tokens * output_cost)`
     - Fall back to existing hardcoded estimates if litellm lookup fails
   - Add provider-aware cost estimation: the model parameter should accept LiteLLM-style model strings (e.g., "openai/gpt-4o", "anthropic/claude-sonnet-4-20250514")
   - Register Ollama local models as zero-cost: input_cost=0, output_cost=0 (they are free)
   - Keep existing Google Gemini pricing as fallback

2. Create `tests/llm/__init__.py` (empty file).

3. Create `tests/llm/test_litellm_provider.py`:

   Test `get_litellm_model_string()`:
   - ("openai", "gpt-4o") -> "openai/gpt-4o"
   - ("anthropic", "claude-sonnet-4-20250514") -> "anthropic/claude-sonnet-4-20250514"
   - ("google", "gemini-2.5-flash") -> "gemini/gemini-2.5-flash"
   - ("ollama", "llama3") -> "ollama_chat/llama3"

   Test `create_litellm_llm()` (mock ChatLiteLLM to avoid real API calls):
   - Mock `langchain_community.chat_models.ChatLiteLLM` import
   - Verify it is called with correct model string and temperature
   - Verify Ollama sets api_base to localhost:11434

   Test `call_litellm_completion()` (mock litellm.completion):
   - Mock `litellm.completion` return value with proper response structure
   - Verify returned dict has content, tokens, cost fields
   - Test error handling: mock APIError, verify it's caught and re-raised properly

4. Update `tests/llm/test_provider.py` (create if doesn't exist at this path, or update existing):

   Test `create_llm` with multi-provider routing:
   - Mock `create_litellm_llm` and verify it's called for provider="openai"
   - Mock `create_litellm_llm` and verify it's called for provider="anthropic"
   - Mock `create_litellm_llm` and verify it's called for provider="google" (when LiteLLM available)
   - Mock `create_litellm_llm` and verify it's called for provider="ollama"
   - Test fallback: when LITELLM_AVAILABLE=False and provider="google", verify `create_google_llm` is called
   - Test error: when LITELLM_AVAILABLE=False and provider="openai", verify LLMProviderError raised
   - Test unsupported provider raises LLMProviderError

   Test config merging still works (existing tests should pass):
   - Global config + node override merging

   Follow existing test patterns: `@patch()` decorators, `unittest.mock`, class-based test grouping.
  </action>
  <verify>
Run `pytest tests/llm/ -v` and confirm all tests pass.
Run `pytest tests/ -v --tb=short` to confirm existing tests still pass (no regressions from provider changes).
  </verify>
  <done>
Cost estimator uses LiteLLM pricing for all providers. Comprehensive tests verify model string mapping, factory routing for all 4 providers, fallback behavior, and error handling. Full test suite passes with zero regressions.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from configurable_agents.llm import create_llm; from configurable_agents.config import LLMConfig; c = LLMConfig(provider='openai', model='gpt-4o'); print('multi-provider OK')"` -- imports work
2. `pytest tests/llm/ -v` -- all LLM tests pass
3. `pytest tests/ -v --tb=short` -- full test suite passes
4. Config validation: `LLMConfig(provider='invalid')` raises ValueError, `LLMConfig(provider='ollama')` succeeds
5. Feature gate: `get_supported_features()['llm']` includes all four providers
</verification>

<success_criteria>
- LiteLLM-based provider handles OpenAI, Anthropic, Google, and Ollama through unified interface
- Factory routes all providers through LiteLLM, with Google fallback when LiteLLM unavailable
- Config schema validates provider field against supported list
- Cost estimation uses LiteLLM pricing with fallback to hardcoded values
- Ollama uses ollama_chat/ prefix and localhost:11434 API base
- Feature gate reflects multi-provider as supported
- All tests pass (new + existing, zero regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-engine/01-02-SUMMARY.md`
</output>
