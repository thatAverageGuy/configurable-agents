---
phase: 01-core-engine
plan: 04
type: execute
wave: 3
depends_on: ["01-01", "01-03"]
files_modified:
  - src/configurable_agents/runtime/executor.py
  - src/configurable_agents/core/node_executor.py
  - src/configurable_agents/storage/__init__.py
  - src/configurable_agents/config/schema.py
  - tests/runtime/test_executor_storage.py
  - tests/core/test_node_executor_metrics.py
autonomous: true

must_haves:
  truths:
    - "When a workflow runs, a WorkflowRunRecord is persisted to storage before execution begins"
    - "After each node completes, an ExecutionStateRecord is saved with the node ID, state snapshot, latency, token count, and cost"
    - "When a workflow completes, the run record is updated with final status, duration, total tokens, and total cost"
    - "When a workflow fails, the run record is updated with 'failed' status and error message"
    - "User can retrieve execution traces (run history + per-node state snapshots) from storage after workflow completion"
  artifacts:
    - path: "src/configurable_agents/runtime/executor.py"
      provides: "Executor with storage integration for run persistence and completion tracking"
      contains: "create_storage_backend"
    - path: "src/configurable_agents/core/node_executor.py"
      provides: "Node executor that saves per-node execution state with metrics to storage"
      contains: "save_state"
    - path: "tests/runtime/test_executor_storage.py"
      provides: "Integration tests for executor-storage workflow run lifecycle"
      contains: "test_run_persisted"
    - path: "tests/core/test_node_executor_metrics.py"
      provides: "Tests for per-node metrics collection and state persistence"
      contains: "test_node_saves_execution_state"
  key_links:
    - from: "src/configurable_agents/runtime/executor.py"
      to: "src/configurable_agents/storage/factory.py"
      via: "creates storage backend from config at execution start"
      pattern: "create_storage_backend"
    - from: "src/configurable_agents/runtime/executor.py"
      to: "src/configurable_agents/storage/base.py"
      via: "persists WorkflowRunRecord before execution, updates on completion/failure"
      pattern: "workflow_run_repo\\.add|workflow_run_repo\\.update_status"
    - from: "src/configurable_agents/core/node_executor.py"
      to: "src/configurable_agents/storage/base.py"
      via: "saves ExecutionStateRecord after each node execution with metrics"
      pattern: "execution_state_repo\\.save_state"
---

<objective>
Integrate the storage abstraction layer (from Plan 01-01) with the runtime executor and node executor to persist workflow runs, track per-node execution state with metrics (latency, tokens, cost), and enable post-execution trace retrieval.

Purpose: OBS-04 (workflow execution traces with detailed metrics) requires that every workflow run and every node execution is persisted with observable metrics. Without this integration, the storage layer exists but is never used -- it is a dead artifact. This plan wires storage into the execution lifecycle so that after any workflow run, users can query the storage backend to see the full execution trace: which nodes ran, in what order, how long each took, how many tokens each consumed, and what each cost.

Output: Updated `executor.py` that creates storage backend, persists run records, and tracks completion/failure. Updated `node_executor.py` that saves per-node execution state with timing, token, and cost metrics. Integration tests proving the full lifecycle.
</objective>

<execution_context>
@C:\Users\ghost\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\ghost\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-core-engine/01-RESEARCH.md
@.planning/phases/01-core-engine/01-01-SUMMARY.md
@.planning/phases/01-core-engine/01-03-SUMMARY.md
@src/configurable_agents/runtime/executor.py
@src/configurable_agents/core/node_executor.py
@src/configurable_agents/storage/base.py
@src/configurable_agents/storage/models.py
@src/configurable_agents/storage/factory.py
@src/configurable_agents/storage/sqlite.py
@src/configurable_agents/config/schema.py
@src/configurable_agents/observability/cost_estimator.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire storage into runtime executor for workflow run persistence</name>
  <files>
    src/configurable_agents/runtime/executor.py
    src/configurable_agents/config/schema.py
    tests/runtime/test_executor_storage.py
  </files>
  <action>
1. Update `src/configurable_agents/runtime/executor.py` to integrate storage:

   **Add storage imports:**
   ```python
   from configurable_agents.storage import create_storage_backend
   from configurable_agents.storage.models import WorkflowRunRecord
   ```

   **Update `run_workflow_from_config()` to persist workflow runs:**

   After Phase 2 (feature gating check) and before Phase 3 (build state model), add storage initialization:

   ```python
   # Phase 2.5: Initialize storage backend (optional, graceful degradation)
   workflow_run_repo = None
   execution_state_repo = None
   storage_config = None
   if config.config and config.config.storage:
       storage_config = config.config.storage
   try:
       workflow_run_repo, execution_state_repo = create_storage_backend(storage_config)
       logger.debug("Storage backend initialized")
   except Exception as e:
       logger.warning(f"Storage backend initialization failed, continuing without persistence: {e}")
   ```

   **Create WorkflowRunRecord before execution (after state initialization, before graph build):**
   ```python
   import uuid
   import json
   from datetime import datetime, timezone

   run_id = None
   if workflow_run_repo:
       try:
           run_id = str(uuid.uuid4())
           run_record = WorkflowRunRecord(
               id=run_id,
               workflow_name=workflow_name,
               status="running",
               config_snapshot=json.dumps(config.model_dump(), default=str),
               inputs=json.dumps(inputs, default=str),
               started_at=datetime.now(timezone.utc),
           )
           workflow_run_repo.add(run_record)
           logger.debug(f"Persisted workflow run: {run_id}")
       except Exception as e:
           logger.warning(f"Failed to persist workflow run record: {e}")
           run_id = None  # Disable further storage ops for this run
   ```

   **Pass execution_state_repo and run_id to graph builder/node executor:**
   Update the `build_graph()` call to pass the execution_state_repo. The cleanest approach: pass it via the tracker or as a new parameter. Since the existing architecture passes a `tracker` object that nodes already receive, extend the approach:
   - Add `execution_state_repo` and `run_id` as optional parameters to `build_graph()`. The graph builder passes them to `make_node_function()`, which passes them to `execute_node()`.
   - Alternatively, attach them to the tracker object: `tracker.execution_state_repo = execution_state_repo; tracker.run_id = run_id`. This avoids changing the graph builder signature. Use this approach since it requires fewer signature changes.

   **Update run record on completion (in the success path of Phase 7):**
   ```python
   if workflow_run_repo and run_id:
       try:
           workflow_run_repo.update_status(run_id, "completed")
           # Also update duration, token, and cost totals
           # Get cost summary from tracker if available
           total_tokens = 0
           total_cost = 0.0
           if tracker.enabled:
               cost_summary = tracker.get_workflow_cost_summary()
               if cost_summary:
                   total_tokens = cost_summary.get('total_tokens', {}).get('total_tokens', 0)
                   total_cost = cost_summary.get('total_cost_usd', 0.0)

           # Update the run record with final metrics
           # Use the repo's get() + manual update pattern since update_status only handles status
           run_record = workflow_run_repo.get(run_id)
           if run_record:
               run_record.completed_at = datetime.now(timezone.utc)
               run_record.duration_seconds = execution_time
               run_record.total_tokens = total_tokens
               run_record.total_cost_usd = total_cost
               run_record.outputs = json.dumps(final_state, default=str)
               # Re-add to persist updates (SQLAlchemy handles this as an update if same PK)
               # OR add an update_record method. The simplest approach: add a dedicated
               # update_run_metrics() method to the repo. For now, use a direct session approach
               # by calling a new method we add below.
       except Exception as e:
           logger.warning(f"Failed to update workflow run record on completion: {e}")
   ```

   **Update run record on failure (in the except block of Phase 7):**
   ```python
   if workflow_run_repo and run_id:
       try:
           workflow_run_repo.update_status(run_id, "failed")
       except Exception as exc:
           logger.warning(f"Failed to update workflow run record on failure: {exc}")
   ```

2. Add `update_run_completion()` method to the abstract repository and SQLite implementation.

   In `src/configurable_agents/storage/base.py`, add to AbstractWorkflowRunRepository:
   ```python
   @abstractmethod
   def update_run_completion(self, run_id: str, status: str, duration_seconds: float,
                             total_tokens: int, total_cost_usd: float,
                             outputs: Optional[str] = None,
                             error_message: Optional[str] = None) -> None:
       """Update a workflow run with completion metrics."""
       raise NotImplementedError
   ```

   NOTE: Plan 01-01 creates the base.py and sqlite.py files. Since this plan runs AFTER 01-01, we modify the files that 01-01 created. The executor should add this method. If for some reason the method is too invasive for the abstract class, alternatively just use `get()` followed by field updates and a new `add()` call. Choose the cleaner approach at execution time.

3. Update `src/configurable_agents/config/schema.py` if needed:
   - Ensure GlobalConfig has a `storage` field (should already exist from Plan 01-01's Task 1). If not present, add it defensively.

4. Create `tests/runtime/test_executor_storage.py`:

   Test the full executor-storage lifecycle using mocked LLM calls and a real SQLite temp database:

   **test_run_persisted_before_execution:**
   - Create a simple 1-node workflow config
   - Mock LLM calls to return canned response
   - Run `run_workflow_from_config()` with a StorageConfig pointing to tmp_path
   - After execution, query storage to verify WorkflowRunRecord exists
   - Verify record has correct workflow_name, status="completed", non-null started_at

   **test_run_updated_on_completion:**
   - Run a successful workflow with storage enabled
   - Verify run record has status="completed", duration_seconds > 0, completed_at is set

   **test_run_updated_on_failure:**
   - Create a workflow config that will fail (e.g., invalid node that raises)
   - Catch the WorkflowExecutionError
   - Verify run record has status="failed"

   **test_storage_failure_does_not_block_execution:**
   - Mock create_storage_backend to raise an exception
   - Verify workflow still runs successfully (graceful degradation)
   - Verify a warning is logged

   **test_no_storage_config_runs_without_persistence:**
   - Run workflow with no storage config at all
   - Verify workflow runs successfully, no errors

   Use `tmp_path` fixture for SQLite database. Use `@patch` for LLM mocking. Follow existing test patterns in `tests/runtime/`.
  </action>
  <verify>
Run `pytest tests/runtime/test_executor_storage.py -v` -- all storage integration tests pass.
Run `pytest tests/ -v --tb=short` -- full test suite passes (no regressions).
  </verify>
  <done>
Executor creates storage backend from config, persists WorkflowRunRecord before execution, updates with completion metrics (status, duration, tokens, cost) on success, and marks as failed on error. Storage failure degrades gracefully without blocking execution. All tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add per-node execution state tracking with metrics to node executor</name>
  <files>
    src/configurable_agents/core/node_executor.py
    src/configurable_agents/storage/__init__.py
    tests/core/test_node_executor_metrics.py
  </files>
  <action>
1. Update `src/configurable_agents/core/node_executor.py` to save per-node execution state with metrics:

   **Add storage-related imports:**
   ```python
   import time
   import json
   ```
   (time may already be imported; add json for serialization)

   **Update `execute_node()` signature:**
   Add optional parameters for storage integration. Since the tracker object is already passed through, attach the storage repos to the tracker (done in Task 1). The node executor reads them from the tracker:

   ```python
   # At the beginning of execute_node(), extract storage from tracker
   execution_state_repo = getattr(tracker, 'execution_state_repo', None) if tracker else None
   run_id = getattr(tracker, 'run_id', None) if tracker else None
   ```

   **Wrap LLM call with timing:**
   Around the existing LLM call in step 6, add timing:
   ```python
   node_start_time = time.time()
   # ... existing LLM call code ...
   node_duration = time.time() - node_start_time
   ```

   **After successful LLM call and state update (between step 6 and step 7), save execution state:**
   ```python
   # 6.5: PERSIST EXECUTION STATE (if storage available)
   if execution_state_repo and run_id:
       try:
           state_snapshot = {
               "node_id": node_id,
               "duration_seconds": round(node_duration, 4),
               "input_tokens": usage.input_tokens,
               "output_tokens": usage.output_tokens,
               "total_tokens": usage.input_tokens + usage.output_tokens,
               "model": model_name,
               "status": "completed",
           }
           # Add cost if available from cost estimator
           try:
               from configurable_agents.observability import CostEstimator
               estimator = CostEstimator()
               cost = estimator.estimate_cost(
                   model=model_name,
                   input_tokens=usage.input_tokens,
                   output_tokens=usage.output_tokens,
               )
               state_snapshot["cost_usd"] = cost.get("total_cost", 0.0) if isinstance(cost, dict) else 0.0
           except Exception:
               state_snapshot["cost_usd"] = 0.0

           # Include the output state values (for trace inspection)
           output_values = {}
           for output_name in node_config.outputs:
               val = getattr(new_state, output_name, None)
               if val is not None:
                   # Truncate large string outputs for storage efficiency
                   str_val = str(val)
                   output_values[output_name] = str_val[:500] if len(str_val) > 500 else str_val
           state_snapshot["outputs"] = output_values

           execution_state_repo.save_state(
               run_id=run_id,
               state_data=state_snapshot,
               node_id=node_id,
           )
           logger.debug(f"Node '{node_id}': Saved execution state (duration={node_duration:.3f}s, tokens={usage.input_tokens + usage.output_tokens})")
       except Exception as e:
           # Storage failure must not break execution
           logger.warning(f"Node '{node_id}': Failed to save execution state: {e}")
   ```

   **Also save state on node failure:**
   In the except block that catches LLMAPIError/ValidationError, add:
   ```python
   if execution_state_repo and run_id:
       try:
           error_state = {
               "node_id": node_id,
               "duration_seconds": round(time.time() - node_start_time, 4),
               "status": "failed",
               "error": str(e)[:500],
           }
           execution_state_repo.save_state(run_id=run_id, state_data=error_state, node_id=node_id)
       except Exception:
           pass  # Double-fault: don't let storage errors mask the real error
   ```

   IMPORTANT: All storage operations in node_executor must be wrapped in try/except. A storage failure must NEVER prevent a node from completing its execution. This is defense-in-depth.

2. Ensure `src/configurable_agents/storage/__init__.py` exports are sufficient. If Task 1 of this plan adds update_run_completion, make sure it is exported.

3. Create `tests/core/test_node_executor_metrics.py`:

   **test_node_saves_execution_state:**
   - Create mock tracker with execution_state_repo (mock) and run_id
   - Execute a node with mocked LLM (return canned response + usage)
   - Verify execution_state_repo.save_state was called with:
     - run_id matching the tracker's run_id
     - node_id matching the node config
     - state_data containing: duration_seconds > 0, input_tokens, output_tokens, total_tokens, status="completed"

   **test_node_saves_cost_metrics:**
   - Same setup as above but also mock CostEstimator
   - Verify state_data contains cost_usd > 0

   **test_node_saves_output_values:**
   - Execute a node that produces output
   - Verify state_data["outputs"] contains the output field names and (truncated) values

   **test_node_saves_state_on_failure:**
   - Mock LLM to raise LLMAPIError
   - Verify execution_state_repo.save_state was called with status="failed" and error message

   **test_storage_failure_does_not_break_node:**
   - Mock execution_state_repo.save_state to raise Exception
   - Verify node still completes successfully and returns updated state
   - Verify warning is logged

   **test_no_storage_runs_normally:**
   - Execute node with tracker that has no execution_state_repo attribute
   - Verify node completes normally (no errors about missing repo)

   Follow existing test patterns in tests/core/. Use `@patch` for LLM and storage mocking. Use class-based test grouping.
  </action>
  <verify>
Run `pytest tests/core/test_node_executor_metrics.py -v` -- all node metrics tests pass.
Run `pytest tests/ -v --tb=short` -- full test suite passes (no regressions).
  </verify>
  <done>
Node executor saves per-node execution state with metrics (latency, tokens, cost, outputs) after each node. Failed nodes record error state. Storage failures never block execution. All tests pass including existing suite.
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/runtime/test_executor_storage.py -v` -- executor-storage integration tests pass
2. `pytest tests/core/test_node_executor_metrics.py -v` -- per-node metrics tests pass
3. `pytest tests/ -v --tb=short` -- full test suite passes (zero regressions)
4. End-to-end: A workflow run with storage config creates a WorkflowRunRecord, saves ExecutionStateRecords per node, and updates final status
5. Graceful degradation: Workflow runs correctly even when storage is unavailable or fails mid-execution
6. Execution trace retrievable: After a run, `execution_state_repo.get_state_history(run_id)` returns per-node records with metrics
</verification>

<success_criteria>
- WorkflowRunRecord persisted before execution begins with status="running"
- ExecutionStateRecord saved after each node with: node_id, duration_seconds, input_tokens, output_tokens, total_tokens, cost_usd, outputs, status
- Run record updated on completion with: status="completed", duration_seconds, total_tokens, total_cost_usd, outputs
- Run record updated on failure with: status="failed", error_message
- Storage failures gracefully degraded (warning logged, execution continues)
- OBS-04 requirement satisfied: full execution traces with per-node metrics are queryable from storage
- All tests pass (new + existing, zero regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-engine/01-04-SUMMARY.md`
</output>
