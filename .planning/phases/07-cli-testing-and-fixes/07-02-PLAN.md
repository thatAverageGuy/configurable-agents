---
phase: 07-cli-testing-and-fixes
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/cli/test_cli_validate_integration.py
  - src/configurable_agents/cli.py
autonomous: true

must_haves:
  truths:
    - "User can run `configurable-agents validate` and configs are validated correctly"
    - "Subprocess tests verify actual CLI invocation (not just mocked function calls)"
    - "Validation errors include specific field names and expected values"
    - "Error messages guide users to fix their config files"
    - "Tests pass on Windows, macOS, and Linux"
  artifacts:
    - path: "tests/cli/test_cli_validate_integration.py"
      provides: "Subprocess integration tests for validate command"
      exports: ["test_validate_valid_config", "test_validate_missing_file", "test_validate_invalid_yaml", "test_validate_missing_required_fields"]
      min_lines: 100
    - path: "src/configurable_agents/cli.py"
      provides: "CLI validate command implementation"
      contains: "def cmd_validate"
  key_links:
    - from: "tests/cli/test_cli_validate_integration.py"
      to: "configurable_agents.cli"
      via: "subprocess.run with sys.executable -m configurable_agents validate"
      pattern: "subprocess\\.run\\(\\[sys\\.executable.*validate"
    - from: "tests/cli/test_cli_validate_integration.py"
      to: "cli.py:cmd_validate"
      via: "Testing validation error messages"
      pattern: "validation|invalid|required"
---

<objective>
Test and fix the CLI `validate` command to ensure configs are validated correctly with clear error messages.

**Purpose:** The `validate` command is users' first check before running workflows. It must catch all config errors and provide specific, actionable guidance. This plan implements subprocess-based integration tests that verify validation works end-to-end.

**Output:**
- New `tests/cli/test_cli_validate_integration.py` with subprocess-based tests
- Verified validation error messages include field names and expected values
- Clear guidance for common validation failures (missing fields, wrong types, etc.)
</objective>

<execution_context>
@C:\Users\ghost\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\ghost\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-cli-testing-and-fixes/07-RESEARCH.md

# Reference: Existing CLI Code
@tests/test_cli.py
@src/configurable_agents/cli.py
@src/configurable_agents/config/validator.py
</context>

<tasks>

<task type="auto">
  <name>Create subprocess integration test file for validate command</name>
  <files>tests/cli/test_cli_validate_integration.py</files>
  <action>
Create a new file `tests/cli/test_cli_validate_integration.py` with subprocess-based integration tests for the `validate` command.

**File structure:**
```python
"""
Integration tests for CLI validate command using subprocess.

Tests actual CLI invocation for config validation.
Catches import errors, entry point bugs, and validation issues.
"""

import subprocess
import sys
from pathlib import Path

import pytest


class TestCLIValidateHelp:
    """Test validate command help and argument parsing."""

    def test_validate_help_shows_usage(self):
        """Test that --help works for validate command."""
        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", "--help"],
            capture_output=True,
            text=True,
        )
        assert result.returncode == 0
        assert "usage:" in result.stdout.lower()

    def test_validate_shows_verbose_option(self):
        """Test validate command supports --verbose flag."""
        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", "--help"],
            capture_output=True,
            text=True,
        )
        assert "--verbose" in result.stdout or "-v" in result.stdout


class TestCLIValidateErrors:
    """Test validate command error handling."""

    def test_validate_missing_file_clear_error(self):
        """Test validate fails with clear error when config file doesn't exist."""
        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", "nonexistent.yaml"],
            capture_output=True,
            text=True,
        )

        assert result.returncode != 0
        # Clear error message
        assert "not found" in result.stderr.lower() or "no such file" in result.stderr.lower()
        # Actionable guidance
        assert any(word in result.stderr.lower() for word in ["yaml", "check", "path", "file", "create"])

    def test_validate_invalid_yaml_syntax(self, tmp_path):
        """Test validate fails clearly on malformed YAML syntax."""
        config_file = tmp_path / "invalid.yaml"
        config_file.write_text("invalid: yaml: content: [unclosed")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        assert result.returncode != 0
        # Should mention YAML or syntax issue
        assert "yaml" in result.stderr.lower() or "syntax" in result.stderr.lower() or "parse" in result.stderr.lower()

    def test_validate_empty_file(self, tmp_path):
        """Test validate fails clearly on empty config file."""
        config_file = tmp_path / "empty.yaml"
        config_file.write_text("")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        assert result.returncode != 0
        # Should indicate empty or invalid config
        assert any(word in result.stderr.lower() for word in ["empty", "invalid", "required", "missing"])


class TestCLIValidateValidConfigs:
    """Test validate command accepts various valid configs."""

    def test_validate_minimal_valid_config(self, tmp_path):
        """Test validate passes on minimal valid config."""
        config_file = tmp_path / "minimal.yaml"
        config_file.write_text("""
schema_version: "1.0"
flow:
  name: minimal_test
state:
  fields: []
nodes: []
edges: []
""")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        assert result.returncode == 0
        # Should indicate success
        assert "valid" in result.stdout.lower() or "success" in result.stdout.lower() or "ok" in result.stdout.lower()

    def test_validate_complete_valid_config(self, tmp_path):
        """Test validate passes on complete valid config."""
        config_file = tmp_path / "complete.yaml"
        config_file.write_text("""
schema_version: "1.0"
flow:
  name: complete_test
  description: A complete test workflow
state:
  fields:
    - name: message
      type: str
      required: true
    - name: count
      type: int
      default: 1
nodes:
  - id: process
    prompt: "Process: {state.message}"
    outputs: [result]
    output_schema:
      type: object
      fields:
        - name: result
          type: str
edges:
  - {from: START, to: process}
  - {from: process, to: END}
""")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        assert result.returncode == 0
        assert "valid" in result.stdout.lower() or "success" in result.stdout.lower()

    def test_validate_verbose_output(self, tmp_path):
        """Test validate --verbose shows detailed validation info."""
        config_file = tmp_path / "test.yaml"
        config_file.write_text("""
schema_version: "1.0"
flow:
  name: test
state:
  fields: []
nodes: []
edges: []
""")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file), "--verbose"],
            capture_output=True,
            text=True,
        )

        assert result.returncode == 0
        # Verbose mode should show more details
        assert len(result.stdout) > 0


class TestCLIValidateInvalidConfigs:
    """Test validate command catches various invalid configs."""

    def test_validate_missing_required_field(self, tmp_path):
        """Test validate catches missing required field in state."""
        config_file = tmp_path / "missing_field.yaml"
        config_file.write_text("""
schema_version: "1.0"
flow:
  name: test
state:
  fields:
    - name: message
      type: str
      # Missing 'required' field
nodes: []
edges: []
""")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        # May pass or fail depending on schema - but error should be clear if fails
        if result.returncode != 0:
            # Should mention the validation issue
            assert any(word in result.stderr.lower() for word in ["required", "missing", "field", "invalid"])

    def test_validate_missing_flow_name(self, tmp_path):
        """Test validate catches missing flow name."""
        config_file = tmp_path / "no_name.yaml"
        config_file.write_text("""
schema_version: "1.0"
flow:
  # Missing name
  description: Test
state:
  fields: []
nodes: []
edges: []
""")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        # Should fail on missing required flow.name
        assert result.returncode != 0
        assert any(word in result.stderr.lower() for word in ["name", "required", "missing"])

    def test_validate_invalid_node_reference(self, tmp_path):
        """Test validate catches invalid node reference in edges."""
        config_file = tmp_path / "bad_edge.yaml"
        config_file.write_text("""
schema_version: "1.0"
flow:
  name: test
state:
  fields: []
nodes:
  - id: node1
    prompt: "Test"
edges:
  - {from: START, to: nonexistent_node}
""")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        # Should catch invalid node reference
        assert result.returncode != 0
        assert any(word in result.stderr.lower() for word in ["node", "reference", "not found", "invalid"])

    def test_validate_cyclic_edges(self, tmp_path):
        """Test validate catches cycles in workflow graph."""
        config_file = tmp_path / "cycle.yaml"
        config_file.write_text("""
schema_version: "1.0"
flow:
  name: test
state:
  fields: []
nodes:
  - id: node1
    prompt: "Test 1"
  - id: node2
    prompt: "Test 2"
edges:
  - {from: node1, to: node2}
  - {from: node2, to: node1}
""")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        # Should detect cycle (no path to END)
        assert result.returncode != 0
        assert any(word in result.stderr.lower() for word in ["cycle", "end", "path", "terminal"])


class TestCLIValidateCrossPlatform:
    """Cross-platform compatibility tests for validate command."""

    def test_validate_path_with_spaces(self, tmp_path):
        """Test that config paths with spaces work on all platforms."""
        config_dir = tmp_path / "my folder" / "sub folder"
        config_dir.mkdir(parents=True)
        config_file = config_dir / "workflow with spaces.yaml"
        config_file.write_text("flow:\n  name: test\nstate:\n  fields: []\nnodes: []\nedges: []\n")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        # Should handle spaces correctly
        assert "not found" not in result.stderr.lower()

    def test_validate_windows_path_separators(self, tmp_path):
        """Test that Windows path separators work correctly."""
        # Create nested directory to test path handling
        config_dir = tmp_path / "nested" / "path"
        config_dir.mkdir(parents=True)
        config_file = config_dir / "config.yaml"
        config_file.write_text("flow:\n  name: test\nstate:\n  fields: []\nnodes: []\nedges: []\n")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        # Should work regardless of path separator
        assert result.returncode == 0 or "not found" not in result.stderr.lower()


class TestCLIValidateErrorMessageQuality:
    """Test that validation error messages are helpful."""

    def test_error_messages_include_field_names(self, tmp_path):
        """Test that validation errors specify which field is problematic."""
        config_file = tmp_path / "test.yaml"
        # Config with obvious issues
        config_file.write_text("flow:\n  # missing name\nstate:\n  fields:\n    - type: str\n      # missing name\n")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        if result.returncode != 0:
            # Error should mention the problematic area
            output = result.stdout.lower() + result.stderr.lower()
            # At minimum should indicate there's a validation problem
            assert any(word in output for word in ["invalid", "required", "missing", "field"])

    def test_error_messages_suggest_fixes(self, tmp_path):
        """Test that error messages suggest how to fix issues."""
        config_file = tmp_path / "missing.yaml"

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        # Should guide user on what to do
        assert any(word in result.stderr.lower() for word in ["check", "file", "path", "create", "yaml"])
```

**Key patterns from RESEARCH.md:**
- Use `subprocess.run([sys.executable, "-m", "configurable_agents", "validate", ...])` for actual CLI invocation
- Test both valid and invalid configs
- Verify error messages are actionable
- Include cross-platform tests for paths with spaces
  </action>
  <verify>Run: python -m pytest tests/cli/test_cli_validate_integration.py -v</verify>
  <done>
    - File created at tests/cli/test_cli_validate_integration.py
    - All test classes and methods defined
    - Tests use subprocess for actual CLI invocation
    - Tests cover valid configs, invalid configs, error messages, and cross-platform paths
  </done>
</task>

<task type="auto">
  <name>Improve validation error messages if needed</name>
  <files>src/configurable_agents/cli.py</files>
  <action>
Review error messages in `cmd_validate` function in `src/configurable_agents/cli.py`.

**Check for actionable guidance:**
- File not found: Should say "Config file 'X' not found. Check the path or create a config file."
- Invalid YAML: Should say "Failed to parse YAML: [error]. Check syntax near line N."
- Missing required field: Should say "Missing required field: [field.name]. Add it to your config."
- Invalid node reference: Should say "Node 'X' referenced in edges but not defined in nodes."

**If messages are vague, improve them:**
1. Import the error formatter utility if available
2. Add specific field names to error messages
3. Include "Fix:" or "To resolve:" guidance

Run the integration tests after any changes to ensure improvements help users.
  </action>
  <verify>Run: python -m pytest tests/cli/test_cli_validate_integration.py::TestCLIValidateErrorMessageQuality -v</verify>
  <done>
    - All error paths in cmd_validate reviewed
    - Error messages include field names where applicable
    - Error messages include actionable guidance ("Fix:", "check", "add", etc.)
    - Tests confirm error message quality
  </done>
</task>

<task type="auto">
  <name>Run integration tests and verify validation works correctly</name>
  <files>tests/cli/test_cli_validate_integration.py</files>
  <action>
Execute the integration tests to verify validation works correctly.

```bash
# Run the new test file
python -m pytest tests/cli/test_cli_validate_integration.py -v

# Run with coverage
python -m pytest tests/cli/test_cli_validate_integration.py --cov=src/configurable_agents/cli --cov-report=term-missing

# Run error message quality tests specifically
python -m pytest tests/cli/test_cli_validate_integration.py::TestCLIValidateErrorMessageQuality -v
```

**Expected results:**
- Help tests pass (test_validate_help_shows_usage, test_validate_shows_verbose_option)
- Error tests pass (test_validate_missing_file_clear_error, test_validate_invalid_yaml_syntax, test_validate_empty_file)
- Valid config tests pass (test_validate_minimal_valid_config, test_validate_complete_valid_config, test_validate_verbose_output)
- Invalid config tests catch errors (test_validate_missing_required_field, test_validate_missing_flow_name, etc.)
- Cross-platform tests pass (test_validate_path_with_spaces, test_validate_windows_path_separators)
- Error message quality tests pass (test_error_messages_include_field_names, test_error_messages_suggest_fixes)
  </action>
  <verify>Run: python -m pytest tests/cli/test_cli_validate_integration.py -v && echo "Tests passed"</verify>
  <done>
    - All tests pass
    - Coverage report shows cmd_validate is well tested
    - Error messages verified to be actionable
    - Validation correctly identifies valid and invalid configs
  </done>
</task>

</tasks>

<verification>
After completing all tasks, verify:

1. New file `tests/cli/test_cli_validate_integration.py` exists with at least 100 lines
2. Tests can be run with `pytest tests/cli/test_cli_validate_integration.py`
3. All help/error/valid config tests pass
4. Invalid config tests correctly catch errors
5. Error messages include field names and actionable guidance
6. Cross-platform path handling tests included
7. Subprocess tests actually invoke the CLI (not just imports)
8. Run `pytest -k "test_cli_validate" --collect-only` to see all tests collected
</verification>

<success_criteria>
Phase 07-02 succeeds when:

1. **CLI-03 satisfied**: User can run `configurable-agents validate` and configs are validated correctly
2. **CLI-06 satisfied**: Validation error messages are clear, actionable, and include resolution steps
3. Subprocess integration tests exist for validate command
4. Validation errors specify exact field names and expected values
5. Tests pass on Windows (path handling works correctly)
6. Coverage of cmd_validate function increased by at least 20%
</success_criteria>

<output>
After completion, create `.planning/phases/07-cli-testing-and-fixes/07-02-SUMMARY.md` with:
- Tests created (count by class)
- Error message improvements made
- Coverage improvement metrics
- Validation edge cases tested
</output>
