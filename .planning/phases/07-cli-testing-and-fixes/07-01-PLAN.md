---
phase: 07-cli-testing-and-fixes
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - tests/cli/test_cli_run_integration.py
  - src/configurable_agents/cli.py
autonomous: true

must_haves:
  truths:
    - "User can run `configurable-agents run` and workflows execute without crashes"
    - "Subprocess tests verify actual CLI invocation (not just mocked function calls)"
    - "UnboundLocalError bug regression test prevents future occurrences"
    - "Error messages include actionable resolution steps"
    - "Tests pass on Windows, macOS, and Linux"
  artifacts:
    - path: "tests/cli/test_cli_run_integration.py"
      provides: "Subprocess integration tests for run command"
      exports: ["test_run_help", "test_run_missing_file", "test_run_invalid_yaml", "test_run_simple_workflow"]
      min_lines: 100
    - path: "src/configurable_agents/cli.py"
      provides: "CLI run command implementation"
      contains: "def cmd_run"
  key_links:
    - from: "tests/cli/test_cli_run_integration.py"
      to: "configurable_agents.cli"
      via: "subprocess.run with sys.executable -m configurable_agents run"
      pattern: "subprocess\\.run\\(\\[sys\\.executable.*run"
    - from: "tests/cli/test_cli_run_integration.py"
      to: "cli.py:cmd_run"
      via: "Verifying Console reference fix"
      pattern: "Console"
---

<objective>
Test and fix the CLI `run` command to ensure workflows execute without crashes.

**Purpose:** The `run` command is the primary user-facing CLI function. A previous UnboundLocalError bug slipped through because tests were heavily mocked. This plan implements subprocess-based integration tests that actually invoke the CLI and catch real bugs.

**Output:**
- New `tests/cli/test_cli_run_integration.py` with subprocess-based tests
- Verified fix for UnboundLocalError (already fixed in Quick-009)
- Clear, actionable error messages for all failure modes
- Regression test to prevent UnboundLocalError from returning
</objective>

<execution_context>
@C:\Users\ghost\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\ghost\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-cli-testing-and-fixes/07-RESEARCH.md
@.planning/phases/05-foundation-reliability/05-01-PLAN.md

# Reference: Existing CLI Code
@tests/test_cli.py
@src/configurable_agents/cli.py
</context>

<tasks>

<task type="auto">
  <name>Create subprocess integration test file for run command</name>
  <files>tests/cli/test_cli_run_integration.py</files>
  <action>
Create a new file `tests/cli/test_cli_run_integration.py` with subprocess-based integration tests for the `run` command.

**File structure:**
```python
"""
Integration tests for CLI run command using subprocess.

Tests actual CLI invocation, not just function imports.
Catches import errors, entry point bugs, and integration issues.
"""

import subprocess
import sys
from pathlib import Path

import pytest


class TestCLIRunHelp:
    """Test run command help and argument parsing."""

    def test_run_help_shows_usage(self):
        """Test that --help works for run command."""
        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "run", "--help"],
            capture_output=True,
            text=True,
        )
        assert result.returncode == 0
        assert "usage:" in result.stdout.lower()

    def test_run_accepts_config_file(self):
        """Test run command accepts config file argument."""
        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "run", "--help"],
            capture_output=True,
            text=True,
        )
        assert "config" in result.stdout.lower() or "positional arguments" in result.stdout.lower()


class TestCLIRunErrors:
    """Test run command error handling."""

    def test_run_missing_file_clear_error(self):
        """Test run fails with clear error when config file doesn't exist."""
        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "run", "nonexistent.yaml"],
            capture_output=True,
            text=True,
        )

        assert result.returncode != 0
        # Clear error message
        assert "not found" in result.stderr.lower() or "no such file" in result.stderr.lower()
        # Actionable guidance
        assert any(word in result.stderr.lower() for word in ["yaml", "check", "path", "file"])

    def test_run_invalid_yaml_syntax(self, tmp_path):
        """Test run fails clearly on malformed YAML syntax."""
        config_file = tmp_path / "invalid.yaml"
        config_file.write_text("invalid: yaml: content: [unclosed")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "run", str(config_file)],
            capture_output=True,
            text=True,
        )

        assert result.returncode != 0
        assert "yaml" in result.stderr.lower() or "syntax" in result.stderr.lower()

    def test_run_invalid_input_format(self, tmp_path):
        """Test run fails on invalid --input format (missing equals)."""
        config_file = tmp_path / "test.yaml"
        config_file.write_text("flow:\n  name: test\n")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "run", str(config_file), "--input", "invalid_no_equals"],
            capture_output=True,
            text=True,
        )

        assert result.returncode != 0
        assert "format" in result.stderr.lower() or "equals" in result.stderr.lower()


class TestCLIRunRegression:
    """Regression tests for previously fixed bugs."""

    def test_unboundlocalerror_regression(self, tmp_path):
        """
        Regression test for UnboundLocalError in cmd_run (Quick-009).

        Bug: Console was referenced in exception handler but not imported
        in all code paths. This test verifies the fix remains in place.
        """
        config_file = tmp_path / "test.yaml"
        # Minimal valid config that triggers the error path
        config_file.write_text("""
schema_version: "1.0"
flow:
  name: test
state:
  fields:
    msg: {type: str, required: true}
nodes:
  - id: test_node
    prompt: "Test {state.msg}"
    outputs: [result]
edges:
  - {from: START, to: test_node}
  - {from: test_node, to: END}
""")

        # Run with valid input - should not raise UnboundLocalError
        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "run", str(config_file), "--input", "msg=hello"],
            capture_output=True,
            text=True,
            timeout=30,
        )

        # Should complete without UnboundLocalError
        assert "UnboundLocalError" not in result.stderr
        assert "local variable 'Console'" not in result.stderr


class TestCLIRunIntegration:
    """Integration tests for run command with real workflows."""

    @pytest.mark.integration
    def test_run_simple_workflow(self, tmp_path):
        """
        Integration test: Run actual simple workflow via CLI.

        Note: This test may fail if API key is not configured,
        but should demonstrate correct parsing and execution flow.
        """
        config_file = tmp_path / "simple.yaml"
        config_file.write_text("""
schema_version: "1.0"
flow:
  name: echo_test
state:
  fields:
    message: {type: str, required: true}
nodes:
  - id: echo
    prompt: "Echo: {state.message}"
    outputs: [result]
    output_schema:
      type: object
      fields:
        - name: result
          type: str
edges:
  - {from: START, to: echo}
  - {from: echo, to: END}
""")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "run", str(config_file), "--input", "message=HelloWorld"],
            capture_output=True,
            text=True,
            timeout=60,
        )

        # May fail due to API key, but should parse correctly
        assert result.returncode in [0, 1]  # 0=success, 1=API error acceptable
        # Check it attempted to run
        assert "echo" in result.stdout.lower() or "api" in result.stderr.lower() or "llm" in result.stderr.lower()


class TestCLIRunCrossPlatform:
    """Cross-platform compatibility tests for run command."""

    def test_run_path_with_spaces(self, tmp_path):
        """Test that config paths with spaces work on all platforms."""
        config_dir = tmp_path / "my folder" / "sub folder"
        config_dir.mkdir(parents=True)
        config_file = config_dir / "workflow with spaces.yaml"
        config_file.write_text("flow:\n  name: test\n")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        # Should handle spaces correctly (validate command for speed)
        # Note: May return validation error, but not "file not found"
        assert "not found" not in result.stderr.lower()

    def test_run_verbose_mode(self, tmp_path):
        """Test run command with --verbose flag."""
        config_file = tmp_path / "test.yaml"
        config_file.write_text("flow:\n  name: test\n")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "run", str(config_file), "--verbose"],
            capture_output=True,
            text=True,
        )

        # Verbose mode should not crash
        # Output may contain debug info, but no UnboundLocalError
        assert "UnboundLocalError" not in result.stderr
```

**Key patterns from RESEARCH.md:**
- Use `subprocess.run([sys.executable, "-m", "configurable_agents", "run", ...])` for actual CLI invocation
- Always set `capture_output=True` and `text=True`
- Use `timeout` parameter for long-running tests
- Test both stdout and stderr
- Include regression test for the UnboundLocalError bug (Quick-009)
  </action>
  <verify>Run: python -m pytest tests/cli/test_cli_run_integration.py -v</verify>
  <done>
    - File created at tests/cli/test_cli_run_integration.py
    - All test classes and methods defined
    - Tests use subprocess for actual CLI invocation
    - UnboundLocalError regression test included
    - Cross-platform path handling tests included
  </done>
</task>

<task type="auto">
  <name>Verify and fix error message clarity in cmd_run</name>
  <files>src/configurable_agents/cli.py</files>
  <action>
Review the `cmd_run` function in `src/configurable_agents/cli.py` to ensure all error messages are clear and actionable.

**Check each error path:**
1. File not found: Should say "Config file 'X' not found. Check the path or create a workflow config."
2. Invalid YAML: Should say "Failed to parse YAML. Check syntax at line N."
3. Validation error: Should say "Config validation failed: [specific error]"
4. Execution error: Should say "Workflow execution failed: [specific error]"

**Current implementation already uses `print_error()`** - verify error messages follow the template:
```
Error: [What went wrong]
Cause: [Why it happened]
Fix: [How to resolve]
```

**Known fix from Quick-009:**
The UnboundLocalError was fixed by ensuring `Console` is imported/available in all code paths. Verify this fix is in place by checking the exception handler in `cmd_run`.

No code changes needed if messages are already clear. Document any improvements needed.
  </action>
  <verify>Run: grep -n "print_error\|raise" src/configurable_agents/cli.py | head -20 to review error handling</verify>
  <done>
    - All error paths in cmd_run reviewed
    - Error messages include actionable guidance
    - UnboundLocalError fix from Quick-009 verified in place
    - Any improvements documented or applied
  </done>
</task>

<task type="auto">
  <name>Run new integration tests and verify they pass</name>
  <files>tests/cli/test_cli_run_integration.py</files>
  <action>
Execute the new integration tests to verify they pass.

```bash
# Run the new test file
python -m pytest tests/cli/test_cli_run_integration.py -v

# Run with coverage
python -m pytest tests/cli/test_cli_run_integration.py --cov=src/configurable_agents/cli --cov-report=term-missing
```

**Expected results:**
- Help tests should always pass (test_run_help_shows_usage, test_run_accepts_config_file)
- Error tests should pass (test_run_missing_file_clear_error, test_run_invalid_yaml_syntax, test_run_invalid_input_format)
- Regression test should pass (test_unboundlocalerror_regression)
- Cross-platform tests should pass (test_run_path_with_spaces, test_run_verbose_mode)
- Integration test may fail if no API key, but that's acceptable (test_run_simple_workflow)

**If tests fail:**
1. Check for actual bugs in cli.py
2. Verify Console import fix is in place
3. Update tests if CLI behavior has changed
4. Ensure error messages are actually actionable
  </action>
  <verify>Run: python -m pytest tests/cli/test_cli_run_integration.py -v && echo "Tests passed"</verify>
  <done>
    - All tests pass (except potentially integration test without API key)
    - Coverage report shows cmd_run is well tested
    - No UnboundLocalError in test output
    - Error messages verified to be actionable
  </done>
</task>

</tasks>

<verification>
After completing all tasks, verify:

1. New file `tests/cli/test_cli_run_integration.py` exists with at least 100 lines
2. Tests can be run with `pytest tests/cli/test_cli_run_integration.py`
3. All help/error tests pass
4. UnboundLocalError regression test exists and passes
5. Cross-platform path handling tests included
6. Subprocess tests actually invoke the CLI (not just imports)
7. Run `pytest -k "test_cli_run" --collect-only` to see all tests collected
</verification>

<success_criteria>
Phase 07-01 succeeds when:

1. **CLI-02 satisfied**: User can run `configurable-agents run` and workflows execute without crashes
2. **CLI-06 satisfied**: Error messages are clear, actionable, and include resolution steps
3. Subprocess integration tests exist for run command
4. UnboundLocalError bug has regression test
5. Tests pass on Windows (subprocess invocation works correctly)
6. Coverage of cmd_run function increased by at least 20%
</success_criteria>

<output>
After completion, create `.planning/phases/07-cli-testing-and-fixes/07-01-SUMMARY.md` with:
- Tests created (count by class)
- Any bugs found and fixed
- Coverage improvement metrics
- Cross-platform verification status
</output>
