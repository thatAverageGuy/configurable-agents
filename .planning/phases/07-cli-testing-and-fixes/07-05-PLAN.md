---
phase: 07-cli-testing-and-fixes
plan: 05
type: execute
wave: 3
depends_on: ["07-01", "07-02", "07-03", "07-04"]
files_modified:
  - tests/cli/test_cli_integration_comprehensive.py
  - tests/cli/conftest.py
  - pyproject.toml
autonomous: true

must_haves:
  truths:
    - "Comprehensive CLI integration tests cover all commands"
    - "Tests use subprocess (not mocked imports) for actual CLI invocation"
    - "Cross-platform tests pass on Windows, macOS, and Linux"
    - "Error message tests verify actionable guidance"
    - "Regression tests prevent previously fixed bugs from returning"
  artifacts:
    - path: "tests/cli/test_cli_integration_comprehensive.py"
      provides: "Comprehensive CLI integration tests"
      exports: ["test_all_commands_help", "test_cross_platform_paths", "test_error_messages"]
      min_lines: 150
    - path: "tests/cli/conftest.py"
      provides: "Shared fixtures for CLI tests"
      exports: ["cli_runner", "sample_config"]
    - path: "pyproject.toml"
      provides: "Test configuration including markers"
      contains: "pytest.ini_config or [tool.pytest"
  key_links:
    - from: "tests/cli/test_cli_integration_comprehensive.py"
      to: "all CLI commands"
      via: "subprocess invocation patterns"
      pattern: "subprocess\\.run\\(\\[sys\\.executable"
    - from: "tests/cli/conftest.py"
      to: "test files"
      via: "pytest fixtures"
      pattern: "@pytest.fixture"
---

<objective>
Add comprehensive CLI integration tests that cover all commands, cross-platform scenarios, and prevent regression of fixed bugs.

**Purpose:** Complete the CLI testing suite with comprehensive integration tests that verify end-to-end functionality across all commands. This plan creates a final integration test file, shared fixtures, and CI configuration to ensure ongoing test reliability.

**Output:**
- Comprehensive `tests/cli/test_cli_integration_comprehensive.py` integration test file
- Shared `tests/cli/conftest.py` fixtures for common test setup
- Updated `pyproject.toml` with pytest configuration for CI
- Full coverage of CLI commands with cross-platform verification
</objective>

<execution_context>
@C:\Users\ghost\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\ghost\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-cli-testing-and-fixes/07-RESEARCH.md

# Reference: Existing Test Files
@tests/test_cli.py
@tests/cli/test_cli_run_integration.py
@tests/cli/test_cli_validate_integration.py
@tests/cli/test_cli_deploy_integration.py
@tests/cli/test_cli_ui_integration.py
</context>

<tasks>

<task type="auto">
  <name>Create comprehensive CLI integration test file</name>
  <files>tests/cli/test_cli_integration_comprehensive.py</files>
  <action>
Create a comprehensive integration test file that covers end-to-end scenarios across all CLI commands.

**File structure:**
```python
"""
Comprehensive integration tests for all CLI commands.

Tests cover:
- All CLI commands work via subprocess
- Cross-platform path handling
- Error message quality
- End-to-end workflows
- Regression prevention for known bugs
"""

import subprocess
import sys
from pathlib import Path

import pytest


class TestAllCommandsHelp:
    """Verify all CLI commands have working help."""

    @pytest.mark.parametrize("command", [
        "run",
        "validate",
        "deploy",
        "ui",
        "dashboard",
        "chat",
    ])
    def test_command_help_exists(self, command):
        """Test each command shows help without crashing."""
        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", command, "--help"],
            capture_output=True,
            text=True,
            timeout=10,
        )
        assert result.returncode == 0, f"Command '{command}' --help failed: {result.stderr}"
        assert "usage:" in result.stdout.lower() or command in result.stdout.lower()


class TestCrossPlatformPaths:
    """Cross-platform path handling tests."""

    def test_config_with_spaces_in_path(self, tmp_path):
        """Test all commands handle paths with spaces correctly."""
        config_dir = tmp_path / "my folder" / "subfolder with spaces"
        config_dir.mkdir(parents=True)
        config_file = config_dir / "config.yaml"
        config_file.write_text("""
schema_version: "1.0"
flow:
  name: test
state:
  fields: []
nodes: []
edges: []
""")

        # Test validate command with spaces
        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        # Should handle spaces correctly
        assert "not found" not in result.stderr.lower()

    def test_config_with_unicode_chars(self, tmp_path):
        """Test all commands handle unicode in paths correctly."""
        config_dir = tmp_path / "tÃ«st" / "folder"
        config_dir.mkdir(parents=True)
        config_file = config_dir / "config.yaml"
        config_file.write_text("flow:\n  name: test\nstate:\n  fields: []\nnodes: []\nedges: []\n")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        # Should handle unicode correctly
        assert result.returncode == 0 or "unicode" not in result.stderr.lower()

    def test_relative_vs_absolute_paths(self, tmp_path):
        """Test commands work with both relative and absolute paths."""
        config_file = tmp_path / "config.yaml"
        config_file.write_text("flow:\n  name: test\nstate:\n  fields: []\nnodes: []\nedges: []\n")

        # Test with absolute path
        result_abs = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file.absolute())],
            capture_output=True,
            text=True,
        )

        # Test with relative path (change to tmp_path directory)
        result_rel = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", "config.yaml"],
            capture_output=True,
            text=True,
            cwd=str(tmp_path),
        )

        # Both should work
        assert result_abs.returncode == result_rel.returncode


class TestErrorMessageQuality:
    """Verify all error messages are actionable."""

    def test_file_not_found_errors_are_actionable(self):
        """Test file not found errors suggest solutions."""
        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", "missing_file.yaml"],
            capture_output=True,
            text=True,
        )

        assert result.returncode != 0
        # Should mention the issue and suggest a fix
        output = result.stderr.lower()
        assert any(word in output for word in ["not found", "no such file"])
        assert any(word in output for word in ["check", "file", "path", "yaml"])

    def test_invalid_yaml_errors_are_specific(self, tmp_path):
        """Test YAML errors indicate the syntax issue."""
        config_file = tmp_path / "invalid.yaml"
        config_file.write_text("invalid: yaml: [unclosed")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        assert result.returncode != 0
        # Should mention YAML/syntax
        output = result.stderr.lower()
        assert any(word in output for word in ["yaml", "syntax", "parse"])

    def test_validation_errors_include_field_names(self, tmp_path):
        """Test validation errors specify which field is problematic."""
        config_file = tmp_path / "test.yaml"
        config_file.write_text("flow:\n  # missing name\n")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )

        if result.returncode != 0:
            # Should indicate what's wrong
            output = result.stderr.lower() + result.stdout.lower()
            assert any(word in output for word in ["required", "missing", "field", "name"])


class TestRegressionTests:
    """Regression tests for previously fixed bugs."""

    def test_unboundlocalerror_regression(self, tmp_path):
        """
        Regression test for UnboundLocalError in cmd_run (Quick-009).

        Bug: Console was referenced in exception handler but not imported
        in all code paths.
        """
        config_file = tmp_path / "test.yaml"
        config_file.write_text("""
schema_version: "1.0"
flow:
  name: test
state:
  fields:
    msg: {type: str, required: true}
nodes:
  - id: test_node
    prompt: "Test {state.msg}"
    outputs: [result]
edges:
  - {from: START, to: test_node}
  - {from: test_node, to: END}
""")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "run", str(config_file), "--input", "msg=hello"],
            capture_output=True,
            text=True,
            timeout=30,
        )

        # Should not have UnboundLocalError
        assert "UnboundLocalError" not in result.stderr
        assert "local variable 'Console'" not in result.stderr

    def test_windows_multiprocessing_regression(self):
        """
        Regression test for Windows multiprocessing issues (Quick-002 to Quick-008).

        Bug: Nested functions and functools.partial don't work with Windows spawn.
        Fix: Module-level functions for all multiprocessing targets.
        """
        import configurable_agents.cli as cli_module

        # Verify module-level functions exist
        assert hasattr(cli_module, "_run_dashboard_with_config")
        assert hasattr(cli_module, "_run_chat_with_config")
        assert callable(cli_module._run_dashboard_with_config)
        assert callable(cli_module._run_chat_with_config)


class TestEndToEndWorkflows:
    """End-to-end workflow tests using multiple CLI commands."""

    def test_validate_then_run_workflow(self, tmp_path):
        """Test typical workflow: validate, then run."""
        config_file = tmp_path / "workflow.yaml"
        config_file.write_text("""
schema_version: "1.0"
flow:
  name: e2e_test
state:
  fields:
    message: {type: str, required: true}
nodes:
  - id: process
    prompt: "Process: {state.message}"
    outputs: [result]
edges:
  - {from: START, to: process}
  - {from: process, to: END}
""")

        # First validate
        validate_result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "validate", str(config_file)],
            capture_output=True,
            text=True,
        )
        assert validate_result.returncode == 0

        # Then run (may fail on API key, but should parse correctly)
        run_result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "run", str(config_file), "--input", "message=test"],
            capture_output=True,
            text=True,
            timeout=60,
        )
        # May fail due to API, but should not crash on parsing
        assert "UnboundLocalError" not in run_result.stderr

    def test_deploy_generate_then_validate(self, tmp_path):
        """Test workflow: deploy generate, validate output."""
        config_file = tmp_path / "workflow.yaml"
        config_file.write_text("""
schema_version: "1.0"
flow:
  name: deploy_test
state:
  fields: []
nodes: []
edges: []
""")

        output_dir = tmp_path / "deploy"
        output_dir.mkdir()

        # Generate deployment artifacts
        deploy_result = subprocess.run(
            [
                sys.executable, "-m", "configurable_agents", "deploy",
                str(config_file), "--generate",
                "--output-dir", str(output_dir)
            ],
            capture_output=True,
            text=True,
            timeout=60,
        )
        assert deploy_result.returncode == 0

        # Verify artifacts created
        artifacts = list(output_dir.iterdir())
        assert len(artifacts) > 0


class TestVerboseMode:
    """Test verbose mode across all commands."""

    @pytest.mark.parametrize("command,extra_args", [
        ("validate", ["--verbose"]),
        ("run", ["--verbose"]),  # Will fail on API but tests verbose flag
    ])
    def test_verbose_mode_does_not_crash(self, tmp_path, command, extra_args):
        """Test verbose mode doesn't cause crashes."""
        config_file = tmp_path / "test.yaml"
        config_file.write_text("flow:\n  name: test\nstate:\n  fields: []\nnodes: []\nedges: []\n")

        args = [sys.executable, "-m", "configurable_agents", command, str(config_file)] + extra_args
        result = subprocess.run(
            args,
            capture_output=True,
            text=True,
            timeout=30,
        )

        # Verbose mode should not crash
        assert "UnboundLocalError" not in result.stderr
        # Should have some output
        assert len(result.stdout) > 0 or len(result.stderr) > 0


class TestInputOutputFormats:
    """Test various input/output formats."""

    def test_input_format_with_equals(self, tmp_path):
        """Test --input format with equals sign."""
        config_file = tmp_path / "test.yaml"
        config_file.write_text("flow:\n  name: test\nstate:\n  fields: []\nnodes: []\nedges: []\n")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "run", str(config_file), "--input", "key=value"],
            capture_output=True,
            text=True,
        )

        # Should parse correctly
        assert "format" not in result.stderr.lower() or result.returncode == 0

    def test_input_format_with_quotes(self, tmp_path):
        """Test --input format with quoted values."""
        config_file = tmp_path / "test.yaml"
        config_file.write_text("flow:\n  name: test\nstate:\n  fields: []\nnodes: []\nedges: []\n")

        result = subprocess.run(
            [sys.executable, "-m", "configurable_agents", "run", str(config_file), "--input", 'message="hello world"'],
            capture_output=True,
            text=True,
        )

        # Should parse correctly
        assert "format" not in result.stderr.lower() or result.returncode == 0
```

**Key patterns:**
- Parametrized tests for multiple commands
- Cross-platform path tests (spaces, unicode, relative/absolute)
- Error message quality verification
- Regression tests for known bugs
- End-to-end workflow tests
  </action>
  <verify>Run: python -m pytest tests/cli/test_cli_integration_comprehensive.py -v</verify>
  <done>
    - File created at tests/cli/test_cli_integration_comprehensive.py
    - All test classes and methods defined
    - Tests cover all CLI commands
    - Cross-platform tests included
    - Regression tests for known bugs included
  </done>
</task>

<task type="auto">
  <name>Create shared conftest.py fixtures for CLI tests</name>
  <files>tests/cli/conftest.py</files>
  <action>
Create a `tests/cli/conftest.py` file with shared fixtures for CLI testing.

**File structure:**
```python
"""
Shared fixtures for CLI integration tests.

Provides common test data and helper functions.
"""

import subprocess
import sys
from pathlib import Path
from typing import List

import pytest


@pytest.fixture
def cli_runner():
    """
    Fixture that provides a function to run CLI commands via subprocess.

    Returns a function that runs commands and returns the result.
    """
    def run_command(args: List[str], timeout: int = 30) -> subprocess.CompletedProcess:
        """Run a CLI command via subprocess."""
        full_args = [sys.executable, "-m", "configurable_agents"] + args
        return subprocess.run(
            full_args,
            capture_output=True,
            text=True,
            timeout=timeout,
        )
    return run_command


@pytest.fixture
def minimal_config(tmp_path):
    """
    Fixture that provides a minimal valid config file.

    Returns a Path object pointing to the config file.
    """
    config_file = tmp_path / "minimal.yaml"
    config_file.write_text("""
schema_version: "1.0"
flow:
  name: minimal_test
state:
  fields: []
nodes: []
edges: []
""")
    return config_file


@pytest.fixture
def complete_config(tmp_path):
    """
    Fixture that provides a complete valid config file.

    Returns a Path object pointing to the config file.
    """
    config_file = tmp_path / "complete.yaml"
    config_file.write_text("""
schema_version: "1.0"
flow:
  name: complete_test
  description: A complete test workflow
state:
  fields:
    - name: message
      type: str
      required: true
    - name: count
      type: int
      default: 1
nodes:
  - id: process
    prompt: "Process: {state.message}"
    outputs: [result]
    output_schema:
      type: object
      fields:
        - name: result
          type: str
edges:
  - {from: START, to: process}
  - {from: process, to: END}
""")
    return config_file


@pytest.fixture
def invalid_yaml_config(tmp_path):
    """
    Fixture that provides an invalid YAML config file.

    Returns a Path object pointing to the config file.
    """
    config_file = tmp_path / "invalid.yaml"
    config_file.write_text("invalid: yaml: content: [unclosed")
    return config_file


@pytest.fixture
def deploy_output_dir(tmp_path):
    """
    Fixture that provides a directory for deploy output.

    Creates and returns a Path object for deployment artifacts.
    """
    output_dir = tmp_path / "deploy_output"
    output_dir.mkdir()
    return output_dir


@pytest.fixture
def requires_api_key():
    """
    Fixture that skips tests if API key is not configured.

    Use this marker for tests that require actual LLM API calls.
    """
    import os
    if not any(
        os.environ.get(key)
        for key in ["OPENAI_API_KEY", "ANTHROPIC_API_KEY", "GOOGLE_API_KEY"]
    ):
        pytest.skip("No API key configured - set OPENAI_API_KEY, ANTHROPIC_API_KEY, or GOOGLE_API_KEY")
```

This conftest.py provides reusable fixtures for all CLI tests.
  </action>
  <verify>Run: python -m pytest tests/cli/test_cli_integration_comprehensive.py::TestEndToEndWorkflows::test_validate_then_run_workflow -v</verify>
  <done>
    - File created at tests/cli/conftest.py
    - All fixtures defined and documented
    - Fixtures provide common test data
    - cli_runner fixture for easy subprocess invocation
  </done>
</task>

<task type="auto">
  <name>Update pyproject.toml with pytest configuration</name>
  <files>pyproject.toml</files>
  <action>
Update `pyproject.toml` to include pytest configuration markers for CLI tests.

**Add or update the `[tool.pytest.ini_options]` section:**

```toml
[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
markers = [
    "integration: marks tests as integration tests (slower, may require external dependencies)",
    "slow: marks tests as slow (may be skipped in CI)",
    "manual: marks tests that require manual verification",
    "requires_api_key: marks tests that require API keys to run"
]
addopts = [
    "-v",
    "--strict-markers",
    "--tb=short",
]
```

**If `pyproject.toml` already has a pytest section, merge the markers:**
- Add any missing markers
- Ensure `integration`, `slow`, `manual` markers are defined
- Add `requires_api_key` marker for tests that need LLM access

This allows running subsets of tests:
- `pytest -m "not slow"` - Skip slow tests
- `pytest -m "not manual"` - Skip manual tests
- `pytest -m "integration"` - Run only integration tests
  </action>
  <verify>Run: pytest --markers to verify markers are registered</verify>
  <done>
    - pyproject.toml updated with pytest configuration
    - Markers defined: integration, slow, manual, requires_api_key
    - `pytest --markers` shows all custom markers
    - Test runners can filter by marker
  </done>
</task>

<task type="auto">
  <name>Run comprehensive CLI tests and verify all pass</name>
  <files>tests/cli/test_cli_integration_comprehensive.py</files>
  <action>
Execute the comprehensive CLI integration tests.

```bash
# Run all CLI integration tests (excluding slow/manual)
python -m pytest tests/cli/test_cli_integration_comprehensive.py -v -m "not slow and not manual"

# Run all CLI tests (including previous plans)
python -m pytest tests/cli/ -v -m "not slow and not manual"

# Run with coverage
python -m pytest tests/cli/ -m "not slow and not manual" --cov=src/configurable_agents/cli --cov-report=term-missing

# Run parametrized help tests
python -m pytest tests/cli/test_cli_integration_comprehensive.py::TestAllCommandsHelp -v

# Run cross-platform tests
python -m pytest tests/cli/test_cli_integration_comprehensive.py::TestCrossPlatformPaths -v

# Run error message quality tests
python -m pytest tests/cli/test_cli_integration_comprehensive.py::TestErrorMessageQuality -v

# Run regression tests
python -m pytest tests/cli/test_cli_integration_comprehensive.py::TestRegressionTests -v
```

**Expected results:**
- All help tests pass for each command
- Cross-platform path tests pass
- Error message tests confirm actionable guidance
- Regression tests pass (UnboundLocalError, Windows multiprocessing)
- End-to-end workflow tests pass
- Coverage shows CLI commands are well tested
  </action>
  <verify>Run: python -m pytest tests/cli/ -v -m "not slow and not manual" --tb=short && echo "All CLI tests passed"</verify>
  <done>
    - All CLI integration tests pass
    - Coverage report shows comprehensive coverage
    - No regressions detected
    - Cross-platform tests verified
    - Error message quality confirmed
  </done>
</task>

</tasks>

<verification>
After completing all tasks, verify:

1. New file `tests/cli/test_cli_integration_comprehensive.py` exists with at least 150 lines
2. New file `tests/cli/conftest.py` exists with fixtures
3. `pyproject.toml` includes pytest configuration with markers
4. All CLI tests can be run with `pytest tests/cli/`
5. Help tests pass for all commands
6. Cross-platform path tests pass
7. Error message quality tests pass
8. Regression tests pass
9. End-to-end workflow tests pass
10. Run `pytest --markers` shows custom markers defined
11. Run `pytest -k "test_cli" --collect-only` to see all CLI tests collected
</verification>

<success_criteria>
Phase 07-05 succeeds when:

1. **CLI-01 through CLI-06 all satisfied**: All CLI requirements met
2. Comprehensive integration tests exist for all commands
3. Tests use subprocess (actual CLI invocation, not imports)
4. Cross-platform tests pass on Windows, macOS, and Linux
5. Error message tests verify actionable guidance
6. Regression tests prevent UnboundLocalError and Windows multiprocessing bugs
7. End-to-end workflow tests demonstrate full CLI functionality
8. CI configuration allows selective test running
9. Overall CLI test coverage exceeds 80%
10. All tests pass consistently
</success_criteria>

<output>
After completion, create `.planning/phases/07-cli-testing-and-fixes/07-05-SUMMARY.md` with:
- Summary of all CLI tests created (count by plan)
- Total test coverage metrics
- Cross-platform verification status
- Regression test coverage
- Known limitations (tests marked as slow/manual)
- Next steps for ongoing test maintenance
</output>
