---
phase: 02-agent-infrastructure
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/configurable_agents/observability/multi_provider_tracker.py
  - src/configurable_agents/observability/mlflow_tracker.py
  - src/configurable_agents/runtime/profiler.py
  - src/configurable_agents/core/node_executor.py
  - src/configurable_agents/runtime/executor.py
  - src/configurable_agents/cli.py
autonomous: true

must_haves:
  truths:
    - "After multi-provider workflow, user sees unified cost breakdown by provider"
    - "Cost report shows total tokens, total cost USD, and per-provider breakdown"
    - "User can identify slowest node through performance profiling output"
    - "Node execution time is tracked in MLFlow metrics"
    - "Bottleneck detection highlights nodes taking >50% of total time"
  artifacts:
    - path: "src/configurable_agents/observability/multi_provider_tracker.py"
      provides: "Multi-provider cost aggregation"
      exports: ["MultiProviderCostTracker", "generate_cost_report"]
      min_lines: 150
    - path: "src/configurable_agents/runtime/profiler.py"
      provides: "Performance profiling decorator"
      exports: ["profile_node", "BottleneckAnalyzer"]
      min_lines: 100
    - path: "src/configurable_agents/core/node_executor.py"
      provides: "Node executor with profiling"
      contains: "profile_node decorator usage"
    - path: "src/configurable_agents/cli.py"
      provides: "CLI commands for cost and profiling reports"
      contains: "cost-report and profile-report commands"
  key_links:
    - from: "src/configurable_agents/runtime/executor.py"
      to: "src/configurable_agents/runtime/profiler.py"
      via: "BottleneckAnalyzer import"
      pattern: "from configurable_agents.runtime.profiler import"
    - from: "src/configurable_agents/observability/multi_provider_tracker.py"
      to: "src/configurable_agents/observability/mlflow_tracker.py"
      via: "MLFlowTracker integration"
      pattern: "MultiProviderCostTracker"
    - from: "src/configurable_agents/core/node_executor.py"
      to: "src/configurable_agents/runtime/profiler.py"
      via: "profile_node decorator"
      pattern: "@profile_node"
---

<objective>
Implement production observability with multi-provider cost tracking and performance profiling.

**Purpose:** Enable users to see unified cost breakdown across all LLM providers (OpenAI, Anthropic, Gemini, Ollama) used in a workflow, and identify performance bottlenecks through node-level timing metrics. This completes the production observability story started in Phase 1 by adding provider-aware cost aggregation and bottleneck detection.

**Output:**
- MultiProviderCostTracker for unified cost reporting across providers
- Performance profiler decorator for node-level timing metrics
- BottleneckAnalyzer to identify slowest nodes
- Integration with existing MLFlowTracker for metric logging
- CLI commands for cost and profiling reports
- Enhanced node_executor with profiling instrumentation
</objective>

<execution_context>
@C:\Users\ghost\.claude\get-shit-done/workflows/execute-plan.md
@C:\Users\ghost\.claude\get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-agent-infrastructure/02-RESEARCH.md

# Existing observability infrastructure (Phase 1)
@src/configurable_agents/observability/mlflow_tracker.py
@src/configurable_agents/observability/cost_estimator.py
@src/configurable_agents/core/node_executor.py
@src/configurable_agents/runtime/executor.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create MultiProviderCostTracker for unified cost reporting</name>
  <files>src/configurable_agents/observability/multi_provider_tracker.py</files>
  <action>
Create multi-provider cost aggregation module:

**Create src/configurable_agents/observability/multi_provider_tracker.py**:

- `MultiProviderCostTracker` class:
  - `__init__(self, mlflow_tracker: MLFlowTracker | None = None)`
  - `track_call(self, provider: str, model: str, response: Any) -> dict`
    - Extracts token usage from LLM response
    - Uses existing CostEstimator for cost calculation
    - Returns dict with: input_tokens, output_tokens, total_tokens, cost_usd, provider, model
    - Handles both LiteLLM response format and direct provider responses
  - `get_cost_summary(self, runs: list) -> dict`
    - Aggregates costs by provider/model combination
    - Returns: {total_cost_usd, total_tokens, by_provider: {provider: {total_cost, total_tokens, call_count}}}
  - `generate_report(self, experiment_name: str) -> dict`
    - Queries MLFlow for all runs in experiment
    - Extracts provider/model from run params and metrics
    - Aggregates costs and returns summary report

- `generate_cost_report(experiment_name: str, mlflow_uri: str | None = None) -> dict`:
  - Standalone function for CLI usage
  - Creates MLflow client, queries experiment, generates report
  - Returns report with: experiment, total_cost_usd, total_tokens, by_provider breakdown

Provider detection logic:
- Extract provider from model name (e.g., "openai/gpt-4o" -> "openai")
- Support providers: openai, anthropic, google, ollama
- Ollama costs are always $0.00 (local models)

Reference: RESEARCH.md Pattern 4 (Multi-Provider Cost Tracking code example), existing CostEstimator in cost_estimator.py.
  </action>
  <verify>
Run: `python -c "from configurable_agents.observability import MultiProviderCostTracker; print('MultiProviderCostTracker imported')"`

Run: `python -c "from configurable_agents.observability.multi_provider_tracker import generate_cost_report; print('generate_cost_report function available')"`
  </verify>
  <done>
MultiProviderCostTracker created with track_call/get_cost_summary/generate_report methods, provider detection handles openai/anthropic/google/ollama, cost aggregation by provider/model complete, generate_cost_report standalone function for CLI.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create performance profiler decorator</name>
  <files>src/configurable_agents/runtime/profiler.py</files>
  <action>
Create performance profiling module:

**Create src/configurable_agents/runtime/profiler.py**:

- `profile_node(node_id: str)` decorator:
  - Wrapper function that times node execution
  - Uses `time.perf_counter()` for high-resolution timing
  - Logs duration_ms to MLFlow as metric: `f"node_{node_id}_duration_ms"`
  - Stores timing in thread-local storage for bottleneck analysis
  - Handles both sync and async functions (via `functools.wraps`)

- `NodeTimings` dataclass:
  - Stores: node_id, duration_ms, timestamp, call_count, total_duration_ms
  - Used for aggregating timing data across workflow execution

- `BottleneckAnalyzer` class:
  - `__init__(self)`
    - Initialize empty timings dict
    - Enable/disable flag (default enabled)
  - `record_node(self, node_id: str, duration_ms: float) -> None`
    - Store or aggregate timing data per node
  - `get_bottlenecks(self, threshold_percent: float = 50.0) -> list[dict]`
    - Calculate total workflow time from all nodes
    - Return nodes contributing >threshold_percent% of total time
    - Each entry: node_id, total_duration_ms, avg_duration_ms, call_count, percent_of_total
  - `get_slowest_node(self) -> dict | None`
    - Return node with highest total_duration_ms
  - `get_summary(self) -> dict`
    - Return: total_time_ms, node_count, slowest_node, bottlenecks

- Global `_profiler_context` (thread-local storage):
  - Store active BottleneckAnalyzer instance
  - Accessor: `get_profiler()` and `set_profiler()`

Reference: RESEARCH.md Pattern 5 (Performance Profiling Decorator code example).
  </action>
  <verify>
Run: `python -c "from configurable_agents.runtime.profiler import profile_node, BottleneckAnalyzer; print('Profiler imported')"`

Run: `python -c "from configurable_agents.runtime.profiler import profile_node; import time; @profile_node('test'); def f(): time.sleep(0.01); f(); print('Decorator works')"`
  </verify>
  <done>
Performance profiler decorator created with sync/async support, BottleneckAnalyzer tracks node timings, bottleneck detection identifies nodes >50% of total time, thread-local context for profiler access, get_slowest_node method returns bottleneck summary.
  </done>
</task>

<task type="auto">
  <name>Task 3: Integrate profiler with node executor</name>
  <files>src/configurable_agents/core/node_executor.py</files>
  <action>
Update node executor to use performance profiling:

**Read existing node_executor.py** and:

1. Add profiler import:
   ```python
   from configurable_agents.runtime.profiler import profile_node, get_profiler, set_profiler, BottleneckAnalyzer
   ```

2. In node execution function (likely `execute_node` or similar):
   - Wrap LLM call with `@profile_node(node_id)` decorator
   - Ensure timing is captured even on exceptions (use try/finally)
   - Store timing info in node output state for analysis

3. Add `BottleneckAnalyzer` lifecycle management:
   - Create analyzer at workflow start in executor
   - Set via `set_profiler(analyzer)` for decorator access
   - Get analyzer at workflow end for bottleneck report

4. Update node output to include timing metadata:
   - Add `_execution_time_ms` field to node output state
   - This will be stored in ExecutionStateRecord for later analysis

5. Integrate with MLFlowTracker:
   - After node execution, log `node_{node_id}_duration_ms` as MLFlow metric
   - This enables MLFlow query-based bottleneck analysis

Reference: Existing node_executor.py patterns, RESEARCH.md Pattern 5 (decorator integration with MLFlow).
  </action>
  <verify>
Run: `python -c "from configurable_agents.core.node_executor import execute_node; print('Node executor imports profiler')"`

Run: Execute a workflow and check MLFlow UI for `node_*_duration_ms` metrics
  </verify>
  <done>
Node executor updated with @profile_node decorator, timing captured for all node executions, BottleneckAnalyzer lifecycle managed in executor, execution_time_ms added to node output state, MLFlow metrics logged per node.
  </done>
</task>

<task type="auto">
  <name>Task 4: Enhance MLFlowTracker with multi-provider tracking</name>
  <files>src/configurable_agents/observability/mlflow_tracker.py</files>
  <action>
Update MLFlowTracker to integrate multi-provider cost tracking:

**Read existing mlflow_tracker.py** and:

1. Add MultiProviderCostTracker integration:
   - Import: `from configurable_agents.observability.multi_provider_tracker import MultiProviderCostTracker`
   - In `__init__`: Create `self.cost_tracker = MultiProviderCostTracker(self)`
   - Add method: `track_provider_call(self, provider: str, model: str, response: Any) -> dict`
     - Delegates to `self.cost_tracker.track_call()`
     - Returns cost dict for immediate use

2. Update `get_workflow_cost_summary()`:
   - Use `MultiProviderCostTracker` for provider-aware aggregation
   - Add `by_provider` field to returned summary
   - Include: provider, model, total_cost_usd, total_tokens, call_count

3. Update `log_workflow_summary()`:
   - Log per-provider costs as MLFlow params: `provider_{name}_cost_usd`
   - Log per-provider token counts as MLFlow params: `provider_{name}_tokens`
   - This enables MLFlow UI filtering and comparison

4. Add new method `get_bottleneck_report(self) -> dict`:
   - Get BottleneckAnalyzer from profiler context
   - Call `get_summary()` and `get_bottlenecks()`
   - Return: slowest_node, bottlenecks, total_time_ms
   - Log bottleneck info to MLFlow as params

Reference: Existing MLFlowTracker patterns, RESEARCH.md Pattern 4 (multi-provider aggregation).
  </action>
  <verify>
Run: `python -c "from configurable_agents.observability import MLFlowTracker; print('MLFlowTracker imports MultiProviderCostTracker')"`

Run: Execute multi-provider workflow and check MLFlow UI for `provider_*_cost_usd` params
  </verify>
  <done>
MLFlowTracker integrates MultiProviderCostTracker, track_provider_call method added, get_workflow_cost_summary returns by_provider breakdown, log_workflow_summary logs per-provider metrics, get_bottleneck_report method provides performance analysis.
  </done>
</task>

<task type="auto">
  <name>Task 5: Update runtime executor for bottleneck reporting</name>
  <files>src/configurable_agents/runtime/executor.py</files>
  <action>
Update runtime executor to collect and report bottlenecks:

**Read existing executor.py** and:

1. In `run_workflow_from_config()`:
   - After graph execution completes, get BottleneckAnalyzer from profiler
   - Call `tracker.log_bottleneck_report(analyzer.get_summary())`
   - Log bottleneck info to console for immediate feedback

2. Add bottleneck summary to workflow completion:
   - Print "Slowest node: {node_id} ({avg_duration_ms:.2f}ms avg)"
   - Print "Bottlenecks (>{threshold}% of total time):"
   - List each bottleneck with percentage

3. Store bottleneck data in WorkflowRunRecord:
   - Add `bottleneck_info` field to record (JSON serialized)
   - Contains: slowest_node, bottlenecks list, total_time_ms
   - Enables historical analysis via storage queries

4. Add CLI flag `--profile` for detailed profiling output:
   - When enabled, print per-node timing table after execution
   - Show: node_id, call_count, total_time_ms, avg_time_ms, percent_of_total
   - Sort by total_time_ms descending

Reference: Existing executor.py completion logging patterns, RESEARCH.md Pattern 5 (bottleneck reporting).
  </action>
  <verify>
Run: `python -m configurable_agents run workflow.yaml --profile`

Verify output includes "Slowest node" and bottleneck table
  </verify>
  <done>
Runtime executor collects bottleneck data from profiler, logs bottleneck report to MLFlow, prints bottleneck summary to console, stores bottleneck info in WorkflowRunRecord, --profile flag enables detailed timing table output.
  </done>
</task>

<task type="auto">
  <name>Task 6: Add CLI commands for cost and profiling reports</name>
  <files>src/configurable_agents/cli.py</files>
  <action>
Add CLI commands for observability reports:

**Update cli.py**:

1. Add `cost-report` command:
   ```bash
   configurable-agents cost-report --experiment <name> --mlflow-uri <uri>
   ```
   - Arguments: `--experiment` (required), `--mlflow-uri` (optional, default from config)
   - Calls `generate_cost_report(experiment_name, mlflow_uri)`
   - Prints formatted table with columns: Provider/Model, Tokens, Cost USD, Calls
   - Shows totals at bottom
   - Highlights most expensive provider

2. Add `profile-report` command:
   ```bash
   configurable-agents profile-report --run-id <id> --mlflow-uri <uri>
   ```
   - Arguments: `--run-id` (optional, default latest), `--mlflow-uri` (optional)
   - Queries MLFlow for `node_*_duration_ms` metrics
   - Prints bottleneck analysis table
   - Shows: Node ID, Avg Duration, Total Duration, Calls, % of Total
   - Highlights slowest node and bottlenecks (>50%)

3. Add `observability` command group:
   ```bash
   configurable-agents observability [--mlflow-uri <uri>]
   ```
   - Subcommands: `cost-report`, `profile-report`, `status`
   - `status` subcommand: Shows MLFlow connection status and recent runs

Use Rich library for formatted tables (already used in project).
Reference: Existing CLI command patterns in cli.py.

4. Add `--enable-profiling` flag to `run` command:
   - When set, enables BottleneckAnalyzer for that run
   - Useful for ad-hoc performance investigation
  </action>
  <verify>
Run: `python -m configurable_agents cost-report --help`

Run: `python -m configurable_agents profile-report --help`

Run: `python -m configurable_agents observability --help`
  </verify>
  <done>
CLI commands added for cost-report/profile-report/observability, Rich tables display cost and timing data, cost-report shows per-provider breakdown, profile-report shows bottleneck analysis, --enable-profiling flag added to run command.
  </done>
</task>

<task type="auto">
  <name>Task 7: Add observability integration tests</name>
  <files>tests/observability/test_multi_provider_tracker.py, tests/observability/test_profiler.py, tests/observability/test_bottleneck_analysis.py</files>
  <action>
Create tests for observability features:

1. **Create tests/observability/test_multi_provider_tracker.py**:
   - Test MultiProviderCostTracker.track_call() with mock responses
   - Test provider detection (openai/gpt-4o, anthropic/claude*, gemini-*, ollama/*)
   - Test cost aggregation by provider
   - Test generate_cost_report() with mock MLFlow data
   - Test Ollama models return $0.00 cost

2. **Create tests/observability/test_profiler.py**:
   - Test @profile_node decorator measures timing accurately
   - Test profiler works with both sync and async functions
   - Test timing stored in thread-local context
   - Test MLFlow metric logging integration

3. **Create tests/observability/test_bottleneck_analysis.py**:
   - Test BottleneckAnalyzer.record_node() aggregates data
   - Test get_bottlenecks() correctly identifies nodes >threshold%
   - Test get_slowest_node() returns highest time node
   - Test get_summary() returns complete analysis
   - Test edge cases (single node, zero duration, negative threshold)

4. **Create tests/observability/test_cli_commands.py**:
   - Test cost-report command with mock MLFlow data
   - Test profile-report command with mock metrics
   - Test observability status command
   - Test Rich table formatting

Use pytest with unittest.mock for MLFlow API mocking.

Reference: Existing test patterns in tests/core/test_parallel.py.
  </action>
  <verify>
Run: `pytest tests/observability/ -v`

Run: `pytest tests/observability/test_multi_provider_tracker.py -k "test_provider_detection" -v`

Run: `pytest tests/observability/test_bottleneck_analysis.py -k "test_bottlenecks" -v`
  </verify>
  <done>
Observability test suite created with multi-provider/profiler/bottleneck/CLI tests, all tests pass, coverage >80% for observability modules, MLFlow mocking correctly simulates API responses.
  </done>
</task>

</tasks>

<verification>
After completing all tasks:

1. **Multi-Provider Cost Verification**:
   - Run workflow using multiple providers (e.g., OpenAI and Gemini nodes)
   - Execute: `python -m configurable_agents cost-report --experiment <experiment_name>`
   - Verify output shows separate entries for each provider with correct totals
   - Check: Total cost = sum of per-provider costs

2. **Ollama Zero-Cost Verification**:
   - Run workflow using only Ollama models
   - Check cost report shows $0.00 for all Ollama calls
   - Verify token counts are still tracked

3. **Bottleneck Detection Verification**:
   - Run workflow with --enable-profiling flag
   - Check console output shows "Slowest node" and bottleneck table
   - Verify bottleneck percentages sum correctly
   - Execute: `python -m configurable_agents profile-report --run-id <id>`
   - Verify MLFlow metrics show per-node duration_ms values

4. **MLFlow Integration Verification**:
   - Open MLFlow UI for the experiment
   - Check Params tab for `provider_*_cost_usd` entries
   - Check Metrics tab for `node_*_duration_ms` entries
   - Verify metrics are queryable and comparable across runs

5. **Storage Verification**:
   - Query WorkflowRunRecord for bottleneck_info field
   - Verify JSON contains: slowest_node, bottlenecks list, total_time_ms
   - Check historical analysis retrieves bottleneck data correctly
</verification>

<success_criteria>
**Phase Success Criteria Met:**
1. After multi-provider workflow, user sees unified cost breakdown by provider (verified via cost-report command)
2. Cost report shows total tokens, total cost USD, and per-provider breakdown (verified via formatted table output)
3. User can identify slowest node through performance profiling output (verified via --profile flag and profile-report command)
4. Node execution time is tracked in MLFlow metrics (verified via MLFlow UI)
5. Bottleneck detection highlights nodes taking >50% of total time (verified via bottleneck table)
</success_criteria>

<output>
After completion, create `.planning/phases/02-agent-infrastructure/02-02-SUMMARY.md` with:
- Frontmatter (phase, plan, wave, status, completed_at, tech_added, patterns_established, key_files)
- Summary of changes (multi-provider tracking, profiling, CLI commands, MLFlow integration)
- Verification results (cost breakdown, bottleneck detection, MLFlow metrics)
- Phase 2 completion summary (both plans complete, all success criteria met)
- Next steps link (Phase 3: Interfaces and Triggers)
</output>
