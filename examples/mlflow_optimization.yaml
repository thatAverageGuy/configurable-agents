# MLFlow Optimization Example
#
# This workflow demonstrates prompt optimization using MLFlow with:
# - A/B testing for prompt variants
# - Quality gates for metric validation
# - MLFlow experiment tracking
#
# Usage:
#   # Run A/B test across all variants
#   configurable-agents optimization ab-test mlflow_optimization.yaml --input topic="AI Safety"
#
#   # After runs complete, evaluate results
#   configurable-agents optimization evaluate --experiment prompt_opt_test
#
#   # Apply best variant to workflow
#   configurable-agents optimization apply-optimized --experiment prompt_opt_test --workflow mlflow_optimization.yaml
#
#   # Or view in dashboard
#   configurable-agents dashboard --mlflow-uri sqlite:///mlflow.db

schema_version: "1.0"

flow:
  name: mlflow_optimization
  description: Prompt optimization workflow with A/B testing and quality gates
  version: "1.0.0"
  tags:
    - optimization
    - mlflow
    - a-b-testing
  author: "Configurable Agents Team"

state:
  fields:
    topic:
      type: str
      required: true
      description: The topic to write about

    article:
      type: str
      default: ""
      description: The generated article

    quality_score:
      type: float
      default: 0.0
      description: Quality assessment score

nodes:
  - id: writer
    description: Generate an article about the topic
    # This prompt will be overridden by A/B test variants
    prompt: |
      Write a short article about {state.topic}.
    outputs: [article]
    output_schema:
      type: object
      fields:
        - name: article
          type: str
          description: The generated article

  - id: quality_check
    description: Evaluate the quality of the generated article
    prompt: |
      Rate the quality of this article about {state.topic}:

      {state.article}

      Provide a score from 0.0 to 1.0 based on:
      - Clarity and coherence
      - Information depth
      - Writing quality

      Return only a number (e.g., 0.85).
    outputs: [quality_score]
    output_schema:
      type: object
      fields:
        - name: quality_score
          type: float
          description: Quality score from 0.0 to 1.0

edges:
  - {from: START, to: writer}
  - {from: writer, to: quality_check}
  - {from: quality_check, to: END}

config:
  # Global MLFlow configuration
  mlflow:
    enabled: true
    tracking_uri: "sqlite:///mlflow.db"
    experiment_name: "prompt_optimization"

  # Default LLM settings
  llm:
    model: "gemini-2.5-flash-lite"
    temperature: 0.7

  execution:
    timeout: 60
    max_retries: 2

  # A/B testing configuration
  ab_test:
    enabled: true
    experiment: "prompt_opt_test"
    run_count: 3  # Run each variant 3 times

    variants:
      # Variant A: Simple direct prompt
      - name: "simple"
        prompt: |
          Write a concise article about {state.topic}.
          Keep it under 200 words.
        node_id: "writer"

      # Variant B: Detailed structured prompt
      - name: "detailed"
        prompt: |
          Write an article about {state.topic} with the following structure:

          1. Introduction: Brief overview of why {state.topic} matters
          2. Key Points: 2-3 main points with explanations
          3. Conclusion: Summary and implications

          Keep each section concise. Total length: 200-300 words.
        node_id: "writer"

      # Variant C: Expert persona prompt
      - name: "expert"
        prompt: |
          You are an expert writer specializing in {state.topic}.

          Write an insightful article about {state.topic} that:
          - Demonstrates deep understanding
          - Uses clear, accessible language
          - Provides actionable takeaways

          Aim for 200-300 words.
        node_id: "writer"

      # Variant D: Question-driven prompt
      - name: "question_driven"
        prompt: |
          Write an article about {state.topic} by answering these questions:
          - What is {state.topic} and why does it matter?
          - What are the key things people should know?
          - What are the practical implications?

          Keep answers concise. Total: 200-300 words.
        node_id: "writer"

  # Quality gates for automatic validation
  gates:
    # Cost thresholds (lower is better)
    - name: "cost_limit"
      description: "Warn if average cost exceeds $0.01 per run"
      enabled: true
      metric: "cost_usd_avg"
      threshold: 0.01  # Max $0.01 per run
      action: "WARN"

    # Latency thresholds (lower is better)
    - name: "latency_limit"
      description: "Warn if average duration exceeds 30 seconds"
      enabled: true
      metric: "duration_ms_avg"
      threshold: 30000  # Max 30 seconds
      action: "WARN"

    # Quality gates (higher is better)
    - name: "quality_minimum"
      description: "Fail if average quality score is below 0.6"
      enabled: true
      metric: "quality_score_avg"
      min_threshold: 0.6  # Minimum 0.6 quality
      action: "FAIL"

    # Block deployment if metrics are too poor
    - name: "deployment_blocker"
      description: "Block deployment if cost is too high"
      enabled: true
      metric: "cost_usd_avg"
      threshold: 0.05
      action: "BLOCK_DEPLOY"

# Webhook registration metadata (optional)
# Register this workflow for webhook triggering:
# POST /webhooks/register
# {
#   "workflow_name": "mlflow_optimization",
#   "description": "Prompt optimization workflow with A/B testing",
#   "allowed_methods": ["generic", "whatsapp", "telegram"],
#   "default_inputs": {"topic": "AI Safety"}
# }
webhook:
  enabled: true
  allowed_methods:
    - generic
    - whatsapp
    - telegram
  default_inputs:
    topic: "AI Safety"

# Example quality gates configuration options:
#
# action options:
#   - "WARN": Log warning but continue
#   - "FAIL": Raise exception and stop execution
#   - "BLOCK_DEPLOY": Set flag to prevent deployment
#
# metric naming conventions:
#   - {metric_name}_avg: Average value across runs
#   - {metric_name}_min: Minimum value
#   - {metric_name}_max: Maximum value
#   - cost_usd_avg: Average cost in USD
#   - duration_ms_avg: Average duration in milliseconds
#   - total_tokens_avg: Average total tokens used
#   - Any custom metric logged to MLFlow
